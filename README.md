# RobustBiasBench

**RobustBiasBench** is a comprehensive benchmark for evaluating the robustness of large language models (LLMs) in detecting social and systemic biases within real-world policy texts. It introduces both a human-annotated dataset of over 18,000 U.S. policy excerpts and a modular perturbation toolkit (`PERTURBKIT`) designed to simulate real-world noise and adversarial attacks.

---

## Project Overview

- **Goal**: Evaluate and improve the robustness of LLMs in identifying **demographic bias**, **systemic bias**, and **no bias** in formal policy documents, even under noisy or adversarial inputs.
- **Dataset**: 18,404 human-annotated excerpts from U.S. government documents (laws, bills, registers, party platforms, etc.)
- **Classes**:
  - `group_1`: Systemic bias (legal, civic, institutional)
  - `group_2`: Demographic bias (identity, social, economic)
  - `no_bias`: Operational or procedural content
- **Perturbation Types**: Typo noise, gibberish prefixes, whitespace alteration, word order shuffling, synonym substitution, prompt injection.

---

## ğŸ“ Repository Structure

```
RobustBiasBench/
â”‚
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ RobustBiasBench Dataset.csv
â”‚   â”œâ”€â”€ Source License Files/
â”‚       â””â”€â”€ BillSum License
â”‚       â””â”€â”€ Federal Regsiter License
â”‚       â””â”€â”€ SPRC19 License
â”‚       â””â”€â”€ Comparative Agendas License
â”‚       â””â”€â”€ source_licenses.txt
â”‚
â”œâ”€â”€ Code/
â”‚   â”œâ”€â”€ Data Curation Code
â”‚       â””â”€â”€ scraping and keywords data curation code divided by dataset
â”‚   â”œâ”€â”€ Models Code
â”‚       â””â”€â”€ LLM fewshot and zeroshot evaluation code divided by model
â”‚   â”œâ”€â”€ PerturbKit Code
â”‚       â””â”€â”€ perturbkit.py
â”‚
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## Evaluation Protocol

### Models Used

- Meta LLaMA3-8B-Instruct
- DeepSeek-7B-Chat
- Mistral-7B-Instruct
- Qwen-2.5-7B-Instruct
- Phi-4-Mini-Instruct
- Vicuna-7B-v1.5

### Prompting Modes

- **Zero-shot**: Instruction + label definitions
- **Few-shot (9-shot)**: 3 examples from each class included in prompt

### Metrics

- Accuracy
- F1 Score
- Precision
- Recall


---

## ğŸ“š Data Sources & Licensing

All datasets are licensed for academic use:

- **Federal Register** â€“ Public Domain
- **Comparative Agendas Project** â€“ CC-BY-NC-SA 4.0
- **BillSum** â€“ CC-BY 4.0 / Apache 2.0
- **SPRC19** â€“ CC0

---

## Limitations

- U.S.-centric dataset â€” may not generalize internationally
- Rule-based perturbations â€” future work may incorporate learned adversarial attacks
- Single-label per excerpt â€” doesn't capture intersectional bias

---

## Contributors

- Akash Bonagiri, Saee Patil, Kevin Xu, Ariana Smith, Gerard Anderias, Andrew Fojas, Winston Yu, Akanksha Bonagiri, Teddy Liu, Setareh Rafatirad

---

## Ethics and Use

This dataset contains excerpts that reference sensitive identities or groups, including but not limited to race, gender, disability, socioeconomic status etc. These texts may reflect biased language present in real-world policy. We **do not endorse** or promote any such biased content; all excerpts are included **solely for research purposes** and should be interpreted within the context of fairness, accountability, and responsible AI development.

---

## Links

- [Codebase](https://github.com/your-org/RobustBiasBench)
- [Dataset Hosting (HuggingFace)](https://huggingface.co/datasets/bsakash/RobustBiasBench)

