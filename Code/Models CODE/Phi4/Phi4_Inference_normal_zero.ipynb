{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and setup (only run once)\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "bias_classes = [\n",
    "    \"no_bias\", \"group_1\", \"group_2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f690f04db0b8485fb69b0792395a2d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.93k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c078122c3d2c42ada23ad945429b8097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/3.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efe252eedc84a36870b09e88d81e76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbb68726bd84cdfae13d4d84d05cbd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/15.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6caef1a0e19844c399181dced3916e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/249 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91435b9157ef404ca8e694c43e2d74eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/587 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255ba3d2e4b842fda13332478b3bc2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5dcfdd1e7d4016afe1a1abd87fed13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/10.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-4-mini-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c191f99808c4b44a965787b7fbd7fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/54.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-4-mini-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d2edc7af1b42c5aa36674a05b4aeeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac173f93a4a4404ace8c9b623ec070e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3adda3a8a83d44c29bb71a9f2ada55fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2da607a0ea44b9bc8cfe1910fdb9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.77G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a04bdb71a6f468183e12bce1d520de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f47dab6d6074574b5b7d65af8a68b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Model loading (only run once)\n",
    "model_name = \"microsoft/Phi-4-mini-instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Cell 3: Prompt(only run once)\n",
    "def classify_bias(description):\n",
    "    class_list = \", \".join(bias_classes)\n",
    "    prompt = [\n",
    "             {\"role\": \"system\", \"content\": (\n",
    "                f\"You are a policy-analysis assistant.\\n\"\n",
    "                f\"Given a policy excerpt, output *exactly one* of these three labels-nothing else, all lower-case:\\n\"\n",
    "                f\"- group_2\\n\"\n",
    "                f\"- group_1\\n\"\n",
    "                f\"- no_bias\\n\\n\"\n",
    "                f\"Definitions:\\n\"\n",
    "                f\"- *group_2: policies related to identity, social, and economic status*, including:\\n\"\n",
    "                f\"• economic: references income, poverty, homelessness, wealth, financial assistance, costs, payments, fees, funding, housing, area living in, etc.\\n\"\n",
    "                f\"• racial_cultural: references race, ethnicity, culture, personal beliefs, etc.\\n\"\n",
    "                f\"• age: references children, youth, child welfare policies, adults, elderly, seniors, age-related policies, etc.\\n\"\n",
    "                f\"• religion: references religious beliefs, regligious groups, faith-based accommodations, etc.\\n\"\n",
    "                f\"• gender: references women, men sex, gender identity, use of only one pronoun for a role, sexual harassment, reproductive rights, pregnancy, etc.\\n\\n\"\n",
    "                f\"- *group_1: policies related to legal, institutional, or civic systems*, including:\\n\"\n",
    "                f\"• political: references voting rights, politics, elections, campaigns, government, war, international relations, etc.\\n\"\n",
    "                f\"• criminal_justice: refernces crime, criminals, court, law enforcement, policing, prison, etc.\\n\"\n",
    "                f\"• citizenship: references immigration, immigration status, deportation, visas, border control, etc.\\n\"\n",
    "                f\"• disability: references physical or mental impairments, accommodations for impairments, accessibility, illness, etc.\\n\"\n",
    "                f\"• education: curriculum, degrees, teaching credentials, language proficiency, language required to learn, standardized testing, school admission, etc.\\n\\n\"\n",
    "                f\"- no_bias: procedural or factual text.\\n\\n\"\n",
    "                f\"Instructions - Follow the following steps exactly, when picking a label:\\n\"\n",
    "                f\"Step 1: Does the excerpt fit under economic, racial_cultural, age, religion, or gender? If yes, pick group_2.\\n\"\n",
    "                f\"Step 2: If the answer to Step 1 was \\\"no\\\", does the excerpt fit under political, criminal_justice, citizenship, disability, or education? If yes, pick group_1.\\n\"\n",
    "                f\"Step 3: If the excerpt does not fit under group_1 and group_2, it is purely a procedure, a definition, or administrative, so pick no_bias.\\n\"\n",
    "                f\"- Do not default to any label. All labels are equally possible for an excerpt.\\n\"\n",
    "                f\"- Return just the label; no explanations.\\n\"\n",
    "                f\"- IMPORTANT: *Avoid defaulting to group_1 or no_bias.* Fully consider if an exerpt is group_2 bias\"\n",
    "             )},\n",
    "            {\"role\": \"user\", \"content\": (\n",
    "                    f\"Classify the following policy excerpt.\\n\"\n",
    "                    f\"Think step by step:\\n\"\n",
    "                    f\"1. Are any groups of people mentioned?\\n\"\n",
    "                    f\"2. Are there any implications of different treatment?\\n\"\n",
    "                    f\"3. Which category does this best fit?\\n\\n\"\n",
    "                    f\"Return *only* one of: group_2, group_1, no_bias\\n\\n\"\n",
    "                    f\"Excerpt:\\n\"\n",
    "                    f\"\\\"\\\"\\\"\\n\"\n",
    "                    f\"{description}\\n\"\n",
    "                    f\"\\\"\\\"\\\"\"\n",
    "            )}\n",
    "     ]\n",
    "    result = pipe(prompt, max_new_tokens=10, do_sample=False)\n",
    "\n",
    "    # The result is a list with a dictionary that has 'generated_text'\n",
    "    full_text = str(result[0]['generated_text'])\n",
    "    #print(\"RAW MODEL OUTPUT:\", full_text)\n",
    "\n",
    "    try:\n",
    "        parsed = ast.literal_eval(full_text)  # Safely evaluate the string as a list of dicts\n",
    "        for entry in parsed:\n",
    "            if entry.get(\"role\") == \"assistant\":\n",
    "                label = entry.get(\"content\", \"\").strip().lower()\n",
    "                if label in {\"group_1\", \"group_2\", \"no_bias\"}:\n",
    "                    return label\n",
    "    except Exception as e:\n",
    "        print(\"Parsing error:\", e)\n",
    "\n",
    "    # Fallback: search for keywords in text\n",
    "    for cls in [\"group_1\", \"group_2\", \"no_bias\"]:\n",
    "        if cls in full_text.lower():\n",
    "            return cls\n",
    "\n",
    "    return \"Unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class distribution:\n",
      "bias_type_merged\n",
      "group_1    7339\n",
      "group_2    6969\n",
      "no_bias    6386\n",
      "Name: count, dtype: int64\n",
      "Processing first 100 rows...\n",
      "\n",
      "===== RESULTS FOR FIRST 100 ROWS =====\n",
      "Accuracy: 60.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no_bias       0.65      0.94      0.77        54\n",
      "     group_1       0.42      0.42      0.42        19\n",
      "     group_2       0.50      0.04      0.07        27\n",
      "\n",
      "    accuracy                           0.60       100\n",
      "   macro avg       0.52      0.47      0.42       100\n",
      "weighted avg       0.56      0.60      0.51       100\n",
      "\n",
      "\n",
      "Detailed Counts and Class Accuracy:\n",
      "         Dataset Count  Predictions Count  Accuracy\n",
      "no_bias             54                 79  0.944444\n",
      "group_1             19                 19  0.421053\n",
      "group_2             27                  2  0.037037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-101-12abe190dd8c>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  first_100['predicted_bias'] = first_100['policy'].apply(classify_bias)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Cell 4: Process data and calculate metrics for first 100 rows\n",
    "# First, run the classification on the dataset\n",
    "df = pd.read_csv(\"FINAL_DATASET.csv\")\n",
    "\n",
    "# Check dataset distribution\n",
    "print(\"Dataset class distribution:\")\n",
    "print(df['bias_type_merged'].value_counts())\n",
    "\n",
    "first_100 = df.head(100)  # Limit to first 100 rows\n",
    "\n",
    "# Process with your classify_bias function\n",
    "print(\"Processing first 100 rows...\")\n",
    "first_100['predicted_bias'] = first_100['policy'].apply(classify_bias)\n",
    "\n",
    "# Ensure we're working with clean labels\n",
    "y_true = first_100['bias_type_merged'].str.strip().str.lower()  # Changed from bias_type to bias_type_merged\n",
    "y_pred = first_100['predicted_bias']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (y_true == y_pred).mean()\n",
    "\n",
    "# Print results in the requested format\n",
    "print(\"\\n===== RESULTS FOR FIRST 100 ROWS =====\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(classification_report(y_true, y_pred, labels=bias_classes))\n",
    "\n",
    "# Also print the custom table format showing counts\n",
    "result_df = pd.DataFrame(index=bias_classes)\n",
    "result_df['Dataset Count'] = [sum(y_true == cls) for cls in bias_classes]\n",
    "result_df['Predictions Count'] = [sum(y_pred == cls) for cls in bias_classes]\n",
    "\n",
    "# Calculate class-wise accuracy\n",
    "for cls in bias_classes:\n",
    "    cls_instances = first_100[y_true == cls]\n",
    "    if len(cls_instances) > 0:\n",
    "        result_df.loc[cls, 'Accuracy'] = sum(cls_instances['predicted_bias'] == cls) / len(cls_instances)\n",
    "    else:\n",
    "        result_df.loc[cls, 'Accuracy'] = 0\n",
    "\n",
    "print(\"\\nDetailed Counts and Class Accuracy:\")\n",
    "print(result_df)\n",
    "\n",
    "# Save results to a CSV file\n",
    "#output_file = \"bias_classification_results_first100.csv\"\n",
    "#first_100.to_csv(output_file, index=False)\n",
    "#print(f\"Results saved to {output_file}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class no_bias has 6386 instances in the full dataset\n",
      "Class group_1 has 7339 instances in the full dataset\n",
      "Class group_2 has 6969 instances in the full dataset\n",
      "\n",
      "Processing balanced sample with 300 rows...\n",
      "\n",
      "===== RESULTS FOR BALANCED SAMPLE =====\n",
      "Accuracy: 57.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no_bias       0.55      0.70      0.62       100\n",
      "     group_1       0.50      0.56      0.53       100\n",
      "     group_2       0.73      0.45      0.56       100\n",
      "\n",
      "    accuracy                           0.57       300\n",
      "   macro avg       0.59      0.57      0.57       300\n",
      "weighted avg       0.59      0.57      0.57       300\n",
      "\n",
      "\n",
      "Detailed Counts and Class Accuracy:\n",
      "         Dataset Count  Predictions Count  Accuracy\n",
      "no_bias            100                127      0.70\n",
      "group_1            100                111      0.56\n",
      "group_2            100                 62      0.45\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Cell 5: Create balanced sample and calculate metrics\n",
    "# Create a balanced sample with 33 rows from each class\n",
    "balanced_df = pd.DataFrame()\n",
    "\n",
    "# Debug class counts in the dataset\n",
    "for cls in bias_classes:\n",
    "    cls_count = sum(df['bias_type_merged'].str.strip().str.lower() == cls)\n",
    "    print(f\"Class {cls} has {cls_count} instances in the full dataset\")\n",
    "\n",
    "for cls in bias_classes:\n",
    "    # Get all rows of this class\n",
    "    cls_rows = df[df['bias_type_merged'].str.strip().str.lower() == cls]\n",
    "\n",
    "    # Sample 100 rows or all available if less\n",
    "    sample_size = min(100, len(cls_rows))\n",
    "    if sample_size < 100:\n",
    "        print(f\"Warning: Only {sample_size} rows available for class {cls}\")\n",
    "\n",
    "    sampled = cls_rows.sample(sample_size, random_state=42)\n",
    "    balanced_df = pd.concat([balanced_df, sampled])\n",
    "\n",
    "# Shuffle the rows\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Process with your classify_bias function\n",
    "print(f\"\\nProcessing balanced sample with {len(balanced_df)} rows...\")\n",
    "balanced_df['predicted_bias'] = balanced_df['policy'].apply(classify_bias)\n",
    "\n",
    "# Ensure we're working with clean labels\n",
    "y_true = balanced_df['bias_type_merged'].str.strip().str.lower()\n",
    "y_pred = balanced_df['predicted_bias']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (y_true == y_pred).mean()\n",
    "\n",
    "# Print results in the requested format\n",
    "print(\"\\n===== RESULTS FOR BALANCED SAMPLE =====\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(classification_report(y_true, y_pred, labels=bias_classes))\n",
    "\n",
    "# Also print the custom table format showing counts\n",
    "balanced_result = pd.DataFrame(index=bias_classes)\n",
    "balanced_result['Dataset Count'] = [sum(y_true == cls) for cls in bias_classes]\n",
    "balanced_result['Predictions Count'] = [sum(y_pred == cls) for cls in bias_classes]\n",
    "\n",
    "# Calculate class-wise accuracy\n",
    "for cls in bias_classes:\n",
    "    cls_instances = balanced_df[y_true == cls]\n",
    "    if len(cls_instances) > 0:\n",
    "        balanced_result.loc[cls, 'Accuracy'] = sum(cls_instances['predicted_bias'] == cls) / len(cls_instances)\n",
    "    else:\n",
    "        balanced_result.loc[cls, 'Accuracy'] = 0\n",
    "\n",
    "print(\"\\nDetailed Counts and Class Accuracy:\")\n",
    "print(balanced_result)\n",
    "\n",
    "# Save results to a CSV file\n",
    "#output_file_balanced = \"bias_classification_results_balanced.csv\"\n",
    "#balanced_df.to_csv(output_file_balanced, index=False)\n",
    "#print(f\"Results saved to {output_file_balanced}\")\n",
    "\n",
    "# Create a download link in Colab\n",
    "#from google.colab import files\n",
    "#files.download(output_file)\n",
    "#files.download(output_file_balanced)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full dataset...\n",
      "Full dataset class distribution:\n",
      "bias_type_merged\n",
      "group_1    7339\n",
      "group_2    6969\n",
      "no_bias    6386\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Processing full dataset in 647 batches of size 32...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca6faa2151b4e3293435fd1ce50695e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "<ipython-input-4-329758d34e53>:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['group_1', 'group_1', 'group_1', 'group_1', 'group_1', 'group_1', 'no_bias', 'no_bias', 'no_bias', 'no_bias', 'no_bias', 'no_bias', 'no_bias', 'no_bias', 'no_bias', 'no_bias', 'no_bias', 'no_bias', 'no_bias', 'no_bias', 'no_bias', 'group_1', 'group_1', 'no_bias', 'no_bias', 'no_bias', 'no_bias', 'group_1', 'no_bias', 'group_1', 'group_1', 'group_1']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[start_idx:end_idx-1, 'predicted_bias'] = batch_predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== RESULTS FOR FULL DATASET =====\n",
      "Total samples processed: 20694\n",
      "Accuracy: 53.99%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no_bias       0.51      0.69      0.59      6386\n",
      "     group_1       0.50      0.52      0.51      7339\n",
      "     group_2       0.67      0.42      0.52      6969\n",
      "\n",
      "    accuracy                           0.54     20694\n",
      "   macro avg       0.56      0.54      0.54     20694\n",
      "weighted avg       0.56      0.54      0.54     20694\n",
      "\n",
      "\n",
      "Detailed Counts and Class Accuracy:\n",
      "         Dataset Count  Predictions Count  Accuracy\n",
      "no_bias           6386               8670  0.689790\n",
      "group_1           7339               7687  0.523777\n",
      "group_2           6969               4337  0.419572\n",
      "\n",
      "Confusion Matrix:\n",
      "        no_bias group_1 group_2\n",
      "no_bias    4405    1538     443\n",
      "group_1    2525    3844     970\n",
      "group_2    1740    2305    2924\n",
      "Results saved to phi4_mini_full_dataset_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Process full dataset with batch processing\n",
    "\n",
    "# Load full dataset\n",
    "print(\"Loading full dataset...\")\n",
    "df = pd.read_csv(\"FINAL_DATASET.csv\")\n",
    "\n",
    "# Check dataset distribution\n",
    "print(\"Full dataset class distribution:\")\n",
    "print(df['bias_type_merged'].value_counts())\n",
    "\n",
    "# Create empty column for predictions\n",
    "df['predicted_bias'] = np.nan\n",
    "\n",
    "# Set up batch processing\n",
    "BATCH_SIZE = 32\n",
    "num_batches = len(df) // BATCH_SIZE + (1 if len(df) % BATCH_SIZE > 0 else 0)\n",
    "\n",
    "# Process in batches with progress bar\n",
    "print(f\"\\nProcessing full dataset in {num_batches} batches of size {BATCH_SIZE}...\")\n",
    "\n",
    "for i in tqdm(range(num_batches)):\n",
    "    # Get batch indices\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, len(df))\n",
    "\n",
    "    # Process batch\n",
    "    batch = df.iloc[start_idx:end_idx]\n",
    "    batch_predictions = [classify_bias(text) for text in batch['policy']]\n",
    "\n",
    "    # Store predictions\n",
    "    df.loc[start_idx:end_idx-1, 'predicted_bias'] = batch_predictions\n",
    "\n",
    "# Ensure we're working with clean labels\n",
    "y_true = df['bias_type_merged'].str.strip().str.lower()\n",
    "y_pred = df['predicted_bias']\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy = (y_true == y_pred).mean()\n",
    "\n",
    "# Print results\n",
    "print(\"\\n===== RESULTS FOR FULL DATASET =====\")\n",
    "print(f\"Total samples processed: {len(df)}\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(classification_report(y_true, y_pred, labels=bias_classes))\n",
    "\n",
    "# Create detailed results table\n",
    "result_df = pd.DataFrame(index=bias_classes)\n",
    "result_df['Dataset Count'] = [sum(y_true == cls) for cls in bias_classes]\n",
    "result_df['Predictions Count'] = [sum(y_pred == cls) for cls in bias_classes]\n",
    "\n",
    "# Calculate class-wise accuracy\n",
    "for cls in bias_classes:\n",
    "    cls_instances = df[y_true == cls]\n",
    "    if len(cls_instances) > 0:\n",
    "        result_df.loc[cls, 'Accuracy'] = sum(cls_instances['predicted_bias'] == cls) / len(cls_instances)\n",
    "    else:\n",
    "        result_df.loc[cls, 'Accuracy'] = 0\n",
    "\n",
    "print(\"\\nDetailed Counts and Class Accuracy:\")\n",
    "print(result_df)\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion_df = pd.DataFrame(index=bias_classes, columns=bias_classes)\n",
    "for true_cls in bias_classes:\n",
    "    for pred_cls in bias_classes:\n",
    "        confusion_df.loc[true_cls, pred_cls] = sum((y_true == true_cls) & (y_pred == pred_cls))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_df)\n",
    "\n",
    "# Save results to a CSV file\n",
    "output_file = \"phi4_mini_full_dataset_results.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
