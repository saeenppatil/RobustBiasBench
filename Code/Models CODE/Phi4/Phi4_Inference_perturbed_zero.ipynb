{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and setup (only run once)\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "bias_classes = [\n",
    "    \"no_bias\", \"group_1\", \"group_2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Model loading (only run once)\n",
    "model_name = \"microsoft/Phi-4-mini-instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Cell 3: Prompt(only run once)\n",
    "def classify_bias(description):\n",
    "    class_list = \", \".join(bias_classes)\n",
    "    prompt = [\n",
    "             {\"role\": \"system\", \"content\": (\n",
    "                f\"You are a policy-analysis assistant.\\n\"\n",
    "                f\"Given a policy excerpt, output *exactly one* of these three labels-nothing else, all lower-case:\\n\"\n",
    "                f\"- group_2\\n\"\n",
    "                f\"- group_1\\n\"\n",
    "                f\"- no_bias\\n\\n\"\n",
    "                f\"Definitions:\\n\"\n",
    "                f\"- *group_2: policies related to identity, social, and economic status*, including:\\n\"\n",
    "                f\"• economic: references income, poverty, homelessness, wealth, financial assistance, costs, payments, fees, funding, housing, area living in, etc.\\n\"\n",
    "                f\"• racial_cultural: references race, ethnicity, culture, personal beliefs, etc.\\n\"\n",
    "                f\"• age: references children, youth, child welfare policies, adults, elderly, seniors, age-related policies, etc.\\n\"\n",
    "                f\"• religion: references religious beliefs, regligious groups, faith-based accommodations, etc.\\n\"\n",
    "                f\"• gender: references women, men sex, gender identity, use of only one pronoun for a role, sexual harassment, reproductive rights, pregnancy, etc.\\n\\n\"\n",
    "                f\"- *group_1: policies related to legal, institutional, or civic systems*, including:\\n\"\n",
    "                f\"• political: references voting rights, politics, elections, campaigns, government, war, international relations, etc.\\n\"\n",
    "                f\"• criminal_justice: refernces crime, criminals, court, law enforcement, policing, prison, etc.\\n\"\n",
    "                f\"• citizenship: references immigration, immigration status, deportation, visas, border control, etc.\\n\"\n",
    "                f\"• disability: references physical or mental impairments, accommodations for impairments, accessibility, illness, etc.\\n\"\n",
    "                f\"• education: curriculum, degrees, teaching credentials, language proficiency, language required to learn, standardized testing, school admission, etc.\\n\\n\"\n",
    "                f\"- no_bias: procedural or factual text.\\n\\n\"\n",
    "                f\"Instructions - Follow the following steps exactly, when picking a label:\\n\"\n",
    "                f\"Step 1: Does the excerpt fit under economic, racial_cultural, age, religion, or gender? If yes, pick group_2.\\n\"\n",
    "                f\"Step 2: If the answer to Step 1 was \\\"no\\\", does the excerpt fit under political, criminal_justice, citizenship, disability, or education? If yes, pick group_1.\\n\"\n",
    "                f\"Step 3: If the excerpt does not fit under group_1 and group_2, it is purely a procedure, a definition, or administrative, so pick no_bias.\\n\"\n",
    "                f\"- Do not default to any label. All labels are equally possible for an excerpt.\\n\"\n",
    "                f\"- Return just the label; no explanations.\\n\"\n",
    "                f\"- IMPORTANT: *Avoid defaulting to group_1 or no_bias.* Fully consider if an exerpt is group_2 bias\"\n",
    "             )},\n",
    "            {\"role\": \"user\", \"content\": (\n",
    "                    f\"Classify the following policy excerpt.\\n\"\n",
    "                    f\"Think step by step:\\n\"\n",
    "                    f\"1. Are any groups of people mentioned?\\n\"\n",
    "                    f\"2. Are there any implications of different treatment?\\n\"\n",
    "                    f\"3. Which category does this best fit?\\n\\n\"\n",
    "                    f\"Return *only* one of: group_2, group_1, no_bias\\n\\n\"\n",
    "                    f\"Excerpt:\\n\"\n",
    "                    f\"\\\"\\\"\\\"\\n\"\n",
    "                    f\"{description}\\n\"\n",
    "                    f\"\\\"\\\"\\\"\"\n",
    "            )}\n",
    "     ]\n",
    "    result = pipe(prompt, max_new_tokens=10, do_sample=False)\n",
    "\n",
    "    # The result is a list with a dictionary that has 'generated_text'\n",
    "    full_text = str(result[0]['generated_text'])\n",
    "    #print(\"RAW MODEL OUTPUT:\", full_text)\n",
    "\n",
    "    try:\n",
    "        parsed = ast.literal_eval(full_text)  # Safely evaluate the string as a list of dicts\n",
    "        for entry in parsed:\n",
    "            if entry.get(\"role\") == \"assistant\":\n",
    "                label = entry.get(\"content\", \"\").strip().lower()\n",
    "                if label in {\"group_1\", \"group_2\", \"no_bias\"}:\n",
    "                    return label\n",
    "    except Exception as e:\n",
    "        print(\"Parsing error:\", e)\n",
    "\n",
    "    # Fallback: search for keywords in text\n",
    "    for cls in [\"group_1\", \"group_2\", \"no_bias\"]:\n",
    "        if cls in full_text.lower():\n",
    "            return cls\n",
    "\n",
    "    return \"Unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Process data and calculate metrics for first 100 rows using perturbed policies\n",
    "# First, run the classification on the dataset\n",
    "df = pd.read_csv(\"FINAL_PERTURBED_DATASET.csv\")\n",
    "\n",
    "# Check dataset distribution\n",
    "print(\"Dataset class distribution:\")\n",
    "print(df['bias_type_merged'].value_counts())\n",
    "\n",
    "first_100 = df.head(100)  # Limit to first 100 rows\n",
    "\n",
    "# Process with your classify_bias function using perturbed policies\n",
    "print(\"Processing first 100 rows (perturbed policies)...\")\n",
    "first_100['predicted_bias'] = first_100['policy_perturbed'].apply(classify_bias)\n",
    "\n",
    "# Ensure we're working with clean labels\n",
    "y_true = first_100['bias_type_merged'].str.strip().str.lower()\n",
    "y_pred = first_100['predicted_bias']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (y_true == y_pred).mean()\n",
    "\n",
    "# Print results in the requested format\n",
    "print(\"\\n===== RESULTS FOR FIRST 100 ROWS (PERTURBED POLICIES) =====\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(classification_report(y_true, y_pred, labels=bias_classes))\n",
    "\n",
    "# Also print the custom table format showing counts\n",
    "result_df = pd.DataFrame(index=bias_classes)\n",
    "result_df['Dataset Count'] = [sum(y_true == cls) for cls in bias_classes]\n",
    "result_df['Predictions Count'] = [sum(y_pred == cls) for cls in bias_classes]\n",
    "\n",
    "# Calculate class-wise accuracy\n",
    "for cls in bias_classes:\n",
    "    cls_instances = first_100[y_true == cls]\n",
    "    if len(cls_instances) > 0:\n",
    "        result_df.loc[cls, 'Accuracy'] = sum(cls_instances['predicted_bias'] == cls) / len(cls_instances)\n",
    "    else:\n",
    "        result_df.loc[cls, 'Accuracy'] = 0\n",
    "\n",
    "print(\"\\nDetailed Counts and Class Accuracy:\")\n",
    "print(result_df)\n",
    "\n",
    "# Save results to a CSV file (optional)\n",
    "# output_file = \"bias_classification_results_first100_perturbed.csv\"\n",
    "# first_100.to_csv(output_file, index=False)\n",
    "# print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Create balanced sample using perturbed policies and calculate metrics\n",
    "# Create a balanced sample with 100 rows from each class\n",
    "balanced_df = pd.DataFrame()\n",
    "\n",
    "# Debug class counts in the dataset\n",
    "for cls in bias_classes:\n",
    "    cls_count = sum(df['bias_type_merged'].str.strip().str.lower() == cls)\n",
    "    print(f\"Class {cls} has {cls_count} instances in the full dataset\")\n",
    "\n",
    "for cls in bias_classes:\n",
    "    # Get all rows of this class\n",
    "    cls_rows = df[df['bias_type_merged'].str.strip().str.lower() == cls]\n",
    "\n",
    "    # Sample 100 rows or all available if less\n",
    "    sample_size = min(100, len(cls_rows))\n",
    "    if sample_size < 100:\n",
    "        print(f\"Warning: Only {sample_size} rows available for class {cls}\")\n",
    "\n",
    "    sampled = cls_rows.sample(sample_size, random_state=42)\n",
    "    balanced_df = pd.concat([balanced_df, sampled])\n",
    "\n",
    "# Shuffle the rows\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Process with your classify_bias function using perturbed policies\n",
    "print(f\"\\nProcessing balanced sample with {len(balanced_df)} rows (perturbed policies)...\")\n",
    "balanced_df['predicted_bias'] = balanced_df['policy_perturbed'].apply(classify_bias)\n",
    "\n",
    "# Ensure we're working with clean labels\n",
    "y_true = balanced_df['bias_type_merged'].str.strip().str.lower()\n",
    "y_pred = balanced_df['predicted_bias']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (y_true == y_pred).mean()\n",
    "\n",
    "# Print results in the requested format\n",
    "print(\"\\n===== RESULTS FOR BALANCED SAMPLE (PERTURBED POLICIES) =====\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(classification_report(y_true, y_pred, labels=bias_classes))\n",
    "\n",
    "# Also print the custom table format showing counts\n",
    "balanced_result = pd.DataFrame(index=bias_classes)\n",
    "balanced_result['Dataset Count'] = [sum(y_true == cls) for cls in bias_classes]\n",
    "balanced_result['Predictions Count'] = [sum(y_pred == cls) for cls in bias_classes]\n",
    "\n",
    "# Calculate class-wise accuracy\n",
    "for cls in bias_classes:\n",
    "    cls_instances = balanced_df[y_true == cls]\n",
    "    if len(cls_instances) > 0:\n",
    "        balanced_result.loc[cls, 'Accuracy'] = sum(cls_instances['predicted_bias'] == cls) / len(cls_instances)\n",
    "    else:\n",
    "        balanced_result.loc[cls, 'Accuracy'] = 0\n",
    "\n",
    "print(\"\\nDetailed Counts and Class Accuracy:\")\n",
    "print(balanced_result)\n",
    "\n",
    "# Save results to a CSV file (optional)\n",
    "# output_file_balanced = \"bias_classification_results_balanced_perturbed.csv\"\n",
    "# balanced_df.to_csv(output_file_balanced, index=False)\n",
    "# print(f\"Results saved to {output_file_balanced}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Process full dataset with batch processing (using perturbed policy excerpts)\n",
    "\n",
    "# Load full dataset\n",
    "print(\"Loading full perturbed dataset...\")\n",
    "df = pd.read_csv(\"FINAL_PERTURBED_DATASET.csv\")\n",
    "\n",
    "# Check dataset distribution\n",
    "print(\"Full dataset class distribution:\")\n",
    "print(df['bias_type_merged'].value_counts())\n",
    "\n",
    "# Create empty column for predictions\n",
    "df['predicted_bias'] = np.nan\n",
    "\n",
    "# Set up batch processing\n",
    "BATCH_SIZE = 32\n",
    "num_batches = len(df) // BATCH_SIZE + (1 if len(df) % BATCH_SIZE > 0 else 0)\n",
    "\n",
    "# Process in batches with progress bar\n",
    "print(f\"\\nProcessing full perturbed dataset in {num_batches} batches of size {BATCH_SIZE}...\")\n",
    "\n",
    "for i in tqdm(range(num_batches)):\n",
    "    # Get batch indices\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, len(df))\n",
    "\n",
    "    # Process batch - using policy_perturbed column instead of policy\n",
    "    batch = df.iloc[start_idx:end_idx]\n",
    "    batch_predictions = [classify_bias(text) for text in batch['policy_perturbed']]\n",
    "\n",
    "    # Store predictions\n",
    "    df.loc[start_idx:end_idx-1, 'predicted_bias'] = batch_predictions\n",
    "\n",
    "# Ensure we're working with clean labels\n",
    "y_true = df['bias_type_merged'].str.strip().str.lower()\n",
    "y_pred = df['predicted_bias']\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy = (y_true == y_pred).mean()\n",
    "\n",
    "# Print results\n",
    "print(\"\\n===== RESULTS FOR FULL PERTURBED DATASET =====\")\n",
    "print(f\"Total samples processed: {len(df)}\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(classification_report(y_true, y_pred, labels=bias_classes))\n",
    "\n",
    "# Create detailed results table\n",
    "result_df = pd.DataFrame(index=bias_classes)\n",
    "result_df['Dataset Count'] = [sum(y_true == cls) for cls in bias_classes]\n",
    "result_df['Predictions Count'] = [sum(y_pred == cls) for cls in bias_classes]\n",
    "\n",
    "# Calculate class-wise accuracy\n",
    "for cls in bias_classes:\n",
    "    cls_instances = df[y_true == cls]\n",
    "    if len(cls_instances) > 0:\n",
    "        result_df.loc[cls, 'Accuracy'] = sum(cls_instances['predicted_bias'] == cls) / len(cls_instances)\n",
    "    else:\n",
    "        result_df.loc[cls, 'Accuracy'] = 0\n",
    "\n",
    "print(\"\\nDetailed Counts and Class Accuracy:\")\n",
    "print(result_df)\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion_df = pd.DataFrame(index=bias_classes, columns=bias_classes)\n",
    "for true_cls in bias_classes:\n",
    "    for pred_cls in bias_classes:\n",
    "        confusion_df.loc[true_cls, pred_cls] = sum((y_true == true_cls) & (y_pred == pred_cls))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_df)\n",
    "\n",
    "# Save results to a CSV file\n",
    "output_file = \"phi4_mini_full_perturbed-zero_results.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
