{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and setup (only run once)\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "bias_classes = [\n",
    "    \"no_bias\", \"group_1\", \"group_2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a594b65cc46c42ea8ba0dfbded3048de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.93k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a29b838fe54bac8d6dff8d47adcd82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/3.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6312de47350d4e1fb31cbae94b1ed62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe1b36f01f46421382a2b86c43b6ff07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/15.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e80739f41324e8b834891a570b7e568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/249 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0bc7b5be4e490f9e0866dce6abb71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/587 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53768f263f7b453ebb0d272e71851248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d511ee8f1edf4d25809cbef4d3bbd0d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/10.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-4-mini-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4839b3cff1e24777b96582f6c7507aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/54.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-4-mini-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c036f171c74b9daff0e5316c8fcfed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50d8bdf574540a5a15bfcdb21146bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acfa9a867277401ab050cc45ecb99de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.77G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5dadda39bb4f02af6b0c960c187feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880521cf5ebd440b9b41cd65c2f454aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871343296b0f43109537759c7c03ac9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Model loading (only run once)\n",
    "model_name = \"microsoft/Phi-4-mini-instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Cell 3: Prompt(only run once)\n",
    "def classify_bias(description):\n",
    "    class_list = \", \".join(bias_classes)\n",
    "    prompt = [\n",
    "             {\"role\": \"system\", \"content\": (\n",
    "                f\"You are a policy-analysis assistant.\\n\"\n",
    "                f\"Given a policy excerpt, output *exactly one* of these three labels-nothing else, all lower-case:\\n\"\n",
    "                f\"- group_2\\n\"\n",
    "                f\"- group_1\\n\"\n",
    "                f\"- no_bias\\n\\n\"\n",
    "                f\"Definitions:\\n\"\n",
    "                f\"- *group_2: policies related to identity, social, and economic status*, including:\\n\"\n",
    "                f\"• economic: references income, poverty, homelessness, wealth, financial assistance, costs, payments, fees, funding, housing, area living in, etc.\\n\"\n",
    "                f\"• racial_cultural: references race, ethnicity, culture, personal beliefs, etc.\\n\"\n",
    "                f\"• age: references children, youth, child welfare policies, adults, elderly, seniors, age-related policies, etc.\\n\"\n",
    "                f\"• religion: references religious beliefs, regligious groups, faith-based accommodations, etc.\\n\"\n",
    "                f\"• gender: references women, men sex, gender identity, use of only one pronoun for a role, sexual harassment, reproductive rights, pregnancy, etc.\\n\\n\"\n",
    "                f\"- *group_1: policies related to legal, institutional, or civic systems*, including:\\n\"\n",
    "                f\"• political: references voting rights, politics, elections, campaigns, government, war, international relations, etc.\\n\"\n",
    "                f\"• criminal_justice: refernces crime, criminals, court, law enforcement, policing, prison, etc.\\n\"\n",
    "                f\"• citizenship: references immigration, immigration status, deportation, visas, border control, etc.\\n\"\n",
    "                f\"• disability: references physical or mental impairments, accommodations for impairments, accessibility, illness, etc.\\n\"\n",
    "                f\"• education: curriculum, degrees, teaching credentials, language proficiency, language required to learn, standardized testing, school admission, etc.\\n\\n\"\n",
    "                f\"- no_bias: procedural or factual text.\\n\\n\"\n",
    "                f\"Instructions - Follow the following steps exactly, when picking a label:\\n\"\n",
    "                f\"Step 1: Does the excerpt fit under economic, racial_cultural, age, religion, or gender? If yes, pick group_2.\\n\"\n",
    "                f\"Step 2: If the answer to Step 1 was \\\"no\\\", does the excerpt fit under political, criminal_justice, citizenship, disability, or education? If yes, pick group_1.\\n\"\n",
    "                f\"Step 3: If the excerpt does not fit under group_1 and group_2, it is purely a procedure, a definition, or administrative, so pick no_bias.\\n\"\n",
    "                f\"- Do not default to any label. All labels are equally possible for an excerpt.\\n\"\n",
    "                f\"- Return just the label; no explanations.\\n\"\n",
    "                f\"- IMPORTANT: *Avoid defaulting to group_1 or no_bias.* Fully consider if an exerpt is group_2 bias\"\n",
    "             )},\n",
    "            {\"role\": \"user\", \"content\": (\n",
    "                    f\"Classify the following policy excerpt.\\n\"\n",
    "                    f\"Think step by step:\\n\"\n",
    "                    f\"1. Are any groups of people mentioned?\\n\"\n",
    "                    f\"2. Are there any implications of different treatment?\\n\"\n",
    "                    f\"3. Which category does this best fit?\\n\\n\"\n",
    "                    f\"Return *only* one of: group_2, group_1, no_bias\\n\\n\"\n",
    "                    f\"Excerpt:\\n\"\n",
    "                    f\"\\\"\\\"\\\"\\n\"\n",
    "                    f\"{description}\\n\"\n",
    "                    f\"\\\"\\\"\\\"\"\n",
    "            )}\n",
    "     ]\n",
    "    result = pipe(prompt, max_new_tokens=10, do_sample=False)\n",
    "\n",
    "    # The result is a list with a dictionary that has 'generated_text'\n",
    "    full_text = str(result[0]['generated_text'])\n",
    "    #print(\"RAW MODEL OUTPUT:\", full_text)\n",
    "\n",
    "    try:\n",
    "        parsed = ast.literal_eval(full_text)  # Safely evaluate the string as a list of dicts\n",
    "        for entry in parsed:\n",
    "            if entry.get(\"role\") == \"assistant\":\n",
    "                label = entry.get(\"content\", \"\").strip().lower()\n",
    "                if label in {\"group_1\", \"group_2\", \"no_bias\"}:\n",
    "                    return label\n",
    "    except Exception as e:\n",
    "        print(\"Parsing error:\", e)\n",
    "\n",
    "    # Fallback: search for keywords in text\n",
    "    for cls in [\"group_1\", \"group_2\", \"no_bias\"]:\n",
    "        if cls in full_text.lower():\n",
    "            return cls\n",
    "\n",
    "    return \"Unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class distribution:\n",
      "bias_type_merged\n",
      "group_1    7339\n",
      "group_2    6969\n",
      "no_bias    6386\n",
      "Name: count, dtype: int64\n",
      "Processing first 100 rows (perturbed policies)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== RESULTS FOR FIRST 100 ROWS (PERTURBED POLICIES) =====\n",
      "Accuracy: 40.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no_bias       0.79      0.35      0.49        54\n",
      "     group_1       0.26      0.42      0.32        19\n",
      "     group_2       0.29      0.48      0.36        27\n",
      "\n",
      "    accuracy                           0.40       100\n",
      "   macro avg       0.45      0.42      0.39       100\n",
      "weighted avg       0.55      0.40      0.42       100\n",
      "\n",
      "\n",
      "Detailed Counts and Class Accuracy:\n",
      "         Dataset Count  Predictions Count  Accuracy\n",
      "no_bias             54                 24  0.351852\n",
      "group_1             19                 31  0.421053\n",
      "group_2             27                 45  0.481481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-c067f25aa052>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  first_100['predicted_bias'] = first_100['policy_perturbed'].apply(classify_bias)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Process data and calculate metrics for first 100 rows using perturbed policies\n",
    "# First, run the classification on the dataset\n",
    "df = pd.read_csv(\"FINAL_PERTURBED_DATASET.csv\")\n",
    "\n",
    "# Check dataset distribution\n",
    "print(\"Dataset class distribution:\")\n",
    "print(df['bias_type_merged'].value_counts())\n",
    "\n",
    "first_100 = df.head(100)  # Limit to first 100 rows\n",
    "\n",
    "# Process with your classify_bias function using perturbed policies\n",
    "print(\"Processing first 100 rows (perturbed policies)...\")\n",
    "first_100['predicted_bias'] = first_100['policy_perturbed'].apply(classify_bias)\n",
    "\n",
    "# Ensure we're working with clean labels\n",
    "y_true = first_100['bias_type_merged'].str.strip().str.lower()\n",
    "y_pred = first_100['predicted_bias']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (y_true == y_pred).mean()\n",
    "\n",
    "# Print results in the requested format\n",
    "print(\"\\n===== RESULTS FOR FIRST 100 ROWS (PERTURBED POLICIES) =====\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(classification_report(y_true, y_pred, labels=bias_classes))\n",
    "\n",
    "# Also print the custom table format showing counts\n",
    "result_df = pd.DataFrame(index=bias_classes)\n",
    "result_df['Dataset Count'] = [sum(y_true == cls) for cls in bias_classes]\n",
    "result_df['Predictions Count'] = [sum(y_pred == cls) for cls in bias_classes]\n",
    "\n",
    "# Calculate class-wise accuracy\n",
    "for cls in bias_classes:\n",
    "    cls_instances = first_100[y_true == cls]\n",
    "    if len(cls_instances) > 0:\n",
    "        result_df.loc[cls, 'Accuracy'] = sum(cls_instances['predicted_bias'] == cls) / len(cls_instances)\n",
    "    else:\n",
    "        result_df.loc[cls, 'Accuracy'] = 0\n",
    "\n",
    "print(\"\\nDetailed Counts and Class Accuracy:\")\n",
    "print(result_df)\n",
    "\n",
    "# Save results to a CSV file (optional)\n",
    "# output_file = \"bias_classification_results_first100_perturbed.csv\"\n",
    "# first_100.to_csv(output_file, index=False)\n",
    "# print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class no_bias has 6386 instances in the full dataset\n",
      "Class group_1 has 7339 instances in the full dataset\n",
      "Class group_2 has 6969 instances in the full dataset\n",
      "\n",
      "Processing balanced sample with 300 rows (perturbed policies)...\n",
      "\n",
      "===== RESULTS FOR BALANCED SAMPLE (PERTURBED POLICIES) =====\n",
      "Accuracy: 43.33%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no_bias       0.45      0.17      0.25       100\n",
      "     group_1       0.42      0.37      0.39       100\n",
      "     group_2       0.44      0.76      0.56       100\n",
      "\n",
      "    accuracy                           0.43       300\n",
      "   macro avg       0.43      0.43      0.40       300\n",
      "weighted avg       0.43      0.43      0.40       300\n",
      "\n",
      "\n",
      "Detailed Counts and Class Accuracy:\n",
      "         Dataset Count  Predictions Count  Accuracy\n",
      "no_bias            100                 38      0.17\n",
      "group_1            100                 89      0.37\n",
      "group_2            100                173      0.76\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Create balanced sample using perturbed policies and calculate metrics\n",
    "# Create a balanced sample with 100 rows from each class\n",
    "balanced_df = pd.DataFrame()\n",
    "\n",
    "# Debug class counts in the dataset\n",
    "for cls in bias_classes:\n",
    "    cls_count = sum(df['bias_type_merged'].str.strip().str.lower() == cls)\n",
    "    print(f\"Class {cls} has {cls_count} instances in the full dataset\")\n",
    "\n",
    "for cls in bias_classes:\n",
    "    # Get all rows of this class\n",
    "    cls_rows = df[df['bias_type_merged'].str.strip().str.lower() == cls]\n",
    "\n",
    "    # Sample 100 rows or all available if less\n",
    "    sample_size = min(100, len(cls_rows))\n",
    "    if sample_size < 100:\n",
    "        print(f\"Warning: Only {sample_size} rows available for class {cls}\")\n",
    "\n",
    "    sampled = cls_rows.sample(sample_size, random_state=42)\n",
    "    balanced_df = pd.concat([balanced_df, sampled])\n",
    "\n",
    "# Shuffle the rows\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Process with your classify_bias function using perturbed policies\n",
    "print(f\"\\nProcessing balanced sample with {len(balanced_df)} rows (perturbed policies)...\")\n",
    "balanced_df['predicted_bias'] = balanced_df['policy_perturbed'].apply(classify_bias)\n",
    "\n",
    "# Ensure we're working with clean labels\n",
    "y_true = balanced_df['bias_type_merged'].str.strip().str.lower()\n",
    "y_pred = balanced_df['predicted_bias']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (y_true == y_pred).mean()\n",
    "\n",
    "# Print results in the requested format\n",
    "print(\"\\n===== RESULTS FOR BALANCED SAMPLE (PERTURBED POLICIES) =====\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(classification_report(y_true, y_pred, labels=bias_classes))\n",
    "\n",
    "# Also print the custom table format showing counts\n",
    "balanced_result = pd.DataFrame(index=bias_classes)\n",
    "balanced_result['Dataset Count'] = [sum(y_true == cls) for cls in bias_classes]\n",
    "balanced_result['Predictions Count'] = [sum(y_pred == cls) for cls in bias_classes]\n",
    "\n",
    "# Calculate class-wise accuracy\n",
    "for cls in bias_classes:\n",
    "    cls_instances = balanced_df[y_true == cls]\n",
    "    if len(cls_instances) > 0:\n",
    "        balanced_result.loc[cls, 'Accuracy'] = sum(cls_instances['predicted_bias'] == cls) / len(cls_instances)\n",
    "    else:\n",
    "        balanced_result.loc[cls, 'Accuracy'] = 0\n",
    "\n",
    "print(\"\\nDetailed Counts and Class Accuracy:\")\n",
    "print(balanced_result)\n",
    "\n",
    "# Save results to a CSV file (optional)\n",
    "# output_file_balanced = \"bias_classification_results_balanced_perturbed.csv\"\n",
    "# balanced_df.to_csv(output_file_balanced, index=False)\n",
    "# print(f\"Results saved to {output_file_balanced}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full perturbed dataset...\n",
      "Full dataset class distribution:\n",
      "bias_type_merged\n",
      "group_1    7339\n",
      "group_2    6969\n",
      "no_bias    6386\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Processing full perturbed dataset in 647 batches of size 32...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022d6804444143d5aa85a6534914d8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-cfe634159bc2>:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['group_1', 'group_1', 'group_2', 'no_bias', 'group_1', 'group_2', 'group_1', 'no_bias', 'group_2', 'no_bias', 'no_bias', 'group_2', 'group_2', 'group_1', 'no_bias', 'no_bias', 'group_2', 'no_bias', 'group_1', 'no_bias', 'group_2', 'group_2', 'group_2', 'group_2', 'group_2', 'group_1', 'group_1', 'group_2', 'no_bias', 'group_2', 'group_1', 'group_1']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[start_idx:end_idx-1, 'predicted_bias'] = batch_predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== RESULTS FOR FULL PERTURBED DATASET =====\n",
      "Total samples processed: 20694\n",
      "Accuracy: 42.96%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     no_bias       0.54      0.19      0.28      6386\n",
      "     group_1       0.43      0.38      0.41      7339\n",
      "     group_2       0.41      0.70      0.51      6969\n",
      "\n",
      "    accuracy                           0.43     20694\n",
      "   macro avg       0.46      0.42      0.40     20694\n",
      "weighted avg       0.46      0.43      0.40     20694\n",
      "\n",
      "\n",
      "Detailed Counts and Class Accuracy:\n",
      "         Dataset Count  Predictions Count  Accuracy\n",
      "no_bias           6386               2275  0.192139\n",
      "group_1           7339               6466  0.382613\n",
      "group_2           6969              11953  0.696657\n",
      "\n",
      "Confusion Matrix:\n",
      "        no_bias group_1 group_2\n",
      "no_bias    1227    1943    3216\n",
      "group_1     649    2808    3882\n",
      "group_2     399    1715    4855\n",
      "Results saved to phi4_mini_full_perturbed-zero_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Process full dataset with batch processing (using perturbed policy excerpts)\n",
    "\n",
    "# Load full dataset\n",
    "print(\"Loading full perturbed dataset...\")\n",
    "df = pd.read_csv(\"FINAL_PERTURBED_DATASET.csv\")\n",
    "\n",
    "# Check dataset distribution\n",
    "print(\"Full dataset class distribution:\")\n",
    "print(df['bias_type_merged'].value_counts())\n",
    "\n",
    "# Create empty column for predictions\n",
    "df['predicted_bias'] = np.nan\n",
    "\n",
    "# Set up batch processing\n",
    "BATCH_SIZE = 32\n",
    "num_batches = len(df) // BATCH_SIZE + (1 if len(df) % BATCH_SIZE > 0 else 0)\n",
    "\n",
    "# Process in batches with progress bar\n",
    "print(f\"\\nProcessing full perturbed dataset in {num_batches} batches of size {BATCH_SIZE}...\")\n",
    "\n",
    "for i in tqdm(range(num_batches)):\n",
    "    # Get batch indices\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, len(df))\n",
    "\n",
    "    # Process batch - using policy_perturbed column instead of policy\n",
    "    batch = df.iloc[start_idx:end_idx]\n",
    "    batch_predictions = [classify_bias(text) for text in batch['policy_perturbed']]\n",
    "\n",
    "    # Store predictions\n",
    "    df.loc[start_idx:end_idx-1, 'predicted_bias'] = batch_predictions\n",
    "\n",
    "# Ensure we're working with clean labels\n",
    "y_true = df['bias_type_merged'].str.strip().str.lower()\n",
    "y_pred = df['predicted_bias']\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy = (y_true == y_pred).mean()\n",
    "\n",
    "# Print results\n",
    "print(\"\\n===== RESULTS FOR FULL PERTURBED DATASET =====\")\n",
    "print(f\"Total samples processed: {len(df)}\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(classification_report(y_true, y_pred, labels=bias_classes))\n",
    "\n",
    "# Create detailed results table\n",
    "result_df = pd.DataFrame(index=bias_classes)\n",
    "result_df['Dataset Count'] = [sum(y_true == cls) for cls in bias_classes]\n",
    "result_df['Predictions Count'] = [sum(y_pred == cls) for cls in bias_classes]\n",
    "\n",
    "# Calculate class-wise accuracy\n",
    "for cls in bias_classes:\n",
    "    cls_instances = df[y_true == cls]\n",
    "    if len(cls_instances) > 0:\n",
    "        result_df.loc[cls, 'Accuracy'] = sum(cls_instances['predicted_bias'] == cls) / len(cls_instances)\n",
    "    else:\n",
    "        result_df.loc[cls, 'Accuracy'] = 0\n",
    "\n",
    "print(\"\\nDetailed Counts and Class Accuracy:\")\n",
    "print(result_df)\n",
    "\n",
    "# Create confusion matrix\n",
    "confusion_df = pd.DataFrame(index=bias_classes, columns=bias_classes)\n",
    "for true_cls in bias_classes:\n",
    "    for pred_cls in bias_classes:\n",
    "        confusion_df.loc[true_cls, pred_cls] = sum((y_true == true_cls) & (y_pred == pred_cls))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_df)\n",
    "\n",
    "# Save results to a CSV file\n",
    "output_file = \"phi4_mini_full_perturbed-zero_results.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
