{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXFF0Hha-WfS"
   },
   "source": [
    "1. Import Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooCo-XCb2xYP"
   },
   "source": [
    "2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9868c385c747413a923b7b72785af69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f391af3db44c35b74387d632c43ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194f3b2df049405abe9e4f8a59e0e7ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0795bc859942c1a175fd2086671bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db92a53202341c5adbb8a223bd6732c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39035287a9c94ac892037a5c80782aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b2769919fe4e44a0b40c435954f8b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0ecf17855d4395ab178bce609197bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf858243f33e43e689cad601a059b9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d60cd14be147df90d1fca5317e7b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5290d6dfe9544396bb6b5ba18c24361e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/162 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = \"lmsys/vicuna-7b-v1.5\"\n",
    "device     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"left\"             # avoid rightâ€‘pad warning\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if device==\"cuda\" else -1,\n",
    "    return_full_text=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "178n5-4G3suG"
   },
   "source": [
    "3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/content/FINAL_PERTURBED_DATASET.csv\")\n",
    "bias_classes = [\"no_bias\",\"group_1\",\"group_2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8z1VIZa30rs"
   },
   "source": [
    "4. Run Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6944e4da7d54472881fb97fd62f189e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 42.37%\n"
     ]
    }
   ],
   "source": [
    "def build_prompt(description: str) -> str:\n",
    "    system = (\n",
    "        \"You are a specialized policy-analysis assistant.\\n\\n\"\n",
    "        \"You will give accurate and helpful answer to user input\\n\"\n",
    "        \"Definitions:\\n\"\n",
    "        \"- A bias policy targets, benefits, burdens, limits, or distinguishes a population based on identity, legal status, or economic position.\\n\"\n",
    "        \"- A non-bias policy is entirely procedural, factual, or operational, and does not involve any group-based treatment\\n\\n\"\n",
    "        \"- group_2 â†’ Policies related to identity or socioeconomic status. Includes:\\n\"\n",
    "        \"- economic: mentions of income, poverty, unemployment assistance, financial assistance, or benefits based on economic standing. This includes housing policies (affordability, assistance programs, rent control), and employment group policies (such as policies affecting nurses or doctors).\\n\"\n",
    "        \"- racial/cultural: references to race, ethnicity, cultural communities, or related terminology. This includes any policy referencing ethnic groups, cultural practices, racial identities, or addressing racial disparities.\\n\"\n",
    "        \"- age: Any policy that refers to elderly individuals, youth, children, age-based programs or limitations. This includes incentive or help for seniors, youth programs, child welfare policies, or any regulation that treats age groups differently.\\n\"\n",
    "        \"- religion: any religious group, practice, or faith-based accommodation or restriction. This includes religious exemptions, exemption for place worship, or regulations affecting religious institutions.\\n\"\n",
    "        \"- gender: references to women, men, gender identity, sexual orientation, sexual discrimination, or gender-based rights. This includes sexual harassment policies, gender equity measures, reproductive rights, protections for gender expression, or LGBTQ+ considerations.\\n\\n\"\n",
    "        \"group_1 â†’ Policies related to civic, institutional, or legal systems. Includes:\\n\"\n",
    "        \"- political: voting rights, voting procedures or regulations, elections, campaigns, or representation. This includes electoral processes, political participation, districting, governmental structures, or international relations.\\n\"\n",
    "        \"- criminal justice: law enforcement, courts, sentencing, prison, or rehabilitation. This includes policing policies, judicial proceedings, corrections, parole systems, or any aspect of the legal enforcement system.\\n\"\n",
    "        \"- citizenship: immigration status, visas, deportation, or naturalization. This includes border policies, citizenship requirements, asylum procedures, or voting rights related to citizenship status.\\n\"\n",
    "        \"- disability: accommodations or treatment for physical, mental, or learning impairments. This includes physical health accommodations, mental health services, accessibility requirements, or any policy that addresses different ability statuses.\\n\"\n",
    "        \"- education: curriculum, school placement, school procedures,learning standards, language proficiency, or educational access. This includes academic policies, language requirements in schools, educational testing, teacher credentials, or school enrollment procedures.\\n\\n\"\n",
    "        \"no_bias â†’ Procedural, administrative, or operational text. Includes:\\n\"\n",
    "        \"- Scheduling, meeting logistics, budgeting language, implementation details, or general operations.\\n\"\n",
    "        \"- Technical definitions, purely factual information, or administrative processes that apply too all group.\\n\\n\"\n",
    "       \"Classification Procedure:\\n\"\n",
    "        \"1. Check for bias policy criteria using the definitions:\\n\"\n",
    "        \"   - Does the text â€˜target, benefit, burden, limit, or distinguishâ€™ a group based on identity (group_2) or legal/economic status (group_1)?\\n\"\n",
    "        \"2. If it meets bias criteria, apply the full category definitions above in priority order:\\n\"\n",
    "        \"   a. Search for any group_2 definition matches (economic, racial/cultural, age, religion, gender). If any apply â†’ label `group_2`.\\n\"\n",
    "        \"   b. Otherwise, search for any group_1 definition matches (political, criminal justice, citizenship, disability, education). If any apply â†’ label `group_1`.\\n\"\n",
    "        \"3. Within the chosen group, select the most specific subcategory when multiple definitions fit.\\n\"\n",
    "        \"4. Only if no group_1 or group_2 definitions apply â†’ label `no_bias`.\\n\"\n",
    "        \"5. Output exactly one label (`group_2`, `group_1`, or `no_bias`) (With no punctuation or variations)\"\n",
    "    )\n",
    "    user = (\n",
    "        f\"Classify the following policy excerpt.\\n\"\n",
    "        f\"Respond with *only one* of the following labels:(`group_2`, `group_1`, or `no_bias`) . No extra explanation or words:\\n\"\n",
    "        f\"Excerpt:\\n\\\"\\\"\\\"\\n{description}\\n\\\"\\\"\\\"\"\n",
    "    )\n",
    "\n",
    "    return f\"{system}\\nUSER: {user}\\nASSISTANT:\"\n",
    "\n",
    "\n",
    "def classify_batch(excerpts: list[str], batch_size: int = 8) -> list[str]:\n",
    "    prompts = [build_prompt(e) for e in excerpts]\n",
    "    preds: list[str] = []\n",
    "\n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Batches\"):\n",
    "        chunk = prompts[i:i+batch_size]\n",
    "\n",
    "        outputs = pipe(\n",
    "            chunk,\n",
    "            max_new_tokens=10,\n",
    "            batch_size=len(chunk),\n",
    "        )\n",
    "        # Flatten\n",
    "        flat = []\n",
    "        for item in outputs:\n",
    "            if isinstance(item, list):\n",
    "                flat.extend(item)\n",
    "            else:\n",
    "                flat.append(item)\n",
    "\n",
    "        for gen in flat:\n",
    "            text = gen[\"generated_text\"]\n",
    "            tok  = text.strip().split()[0].lower().replace(\"\\\\\", \"\")\n",
    "            preds.append(tok)\n",
    "\n",
    "    return preds\n",
    "\n",
    "df[\"predicted_bias\"] = classify_batch(df[\"policy_perturbed\"].tolist(), batch_size=8)\n",
    "df[\"correct\"] = (\n",
    "    df[\"bias_type_merged\"].str.strip().str.lower()\n",
    "    == df[\"predicted_bias\"]\n",
    ")\n",
    "print(f\"\\nAccuracy: {df['correct'].mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEjk9WZgjzxf"
   },
   "source": [
    "Show Ouput Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 42.37%\n",
      "\n",
      "Assembled metrics per class:\n",
      "\n",
      "         precision    recall  f1_score  support\n",
      "group_1   0.460247  0.212972  0.291197     7339\n",
      "group_2   0.386843  0.697805  0.497748     6969\n",
      "no_bias   0.501498  0.366896  0.423766     6386\n",
      "\n",
      "Perâ€‘class accuracy (%):\n",
      "\n",
      "                  total  correct   accuracy\n",
      "bias_type_merged                           \n",
      "group_1            7339     1563  21.297179\n",
      "group_2            6969     4863  69.780456\n",
      "no_bias            6386     2343  36.689634\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "print(f\"\\nAccuracy: {df['correct'].mean() * 100:.2f}%\")\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    df['bias_type_merged'],\n",
    "    df['predicted_bias'],\n",
    "    labels=sorted(df['bias_type_merged'].unique()),\n",
    ")\n",
    "metrics_df = pd.DataFrame({\n",
    "    'precision': precision,\n",
    "    'recall':    recall,\n",
    "    'f1_score':  f1,\n",
    "    'support':   support\n",
    "}, index=sorted(df['bias_type_merged'].unique()))\n",
    "print(\"\\nAssembled metrics per class:\\n\")\n",
    "print(metrics_df)\n",
    "\n",
    "# Perâ€‘class accuracy:\n",
    "class_stats = (\n",
    "    df\n",
    "    .groupby('bias_type_merged')\n",
    "    .agg(\n",
    "        total   = ('correct', 'count'),\n",
    "        correct = ('correct', 'sum')\n",
    "    )\n",
    ")\n",
    "class_stats['accuracy'] = class_stats['correct'] / class_stats['total'] * 100\n",
    "print(\"\\nPerâ€‘class accuracy (%):\\n\")\n",
    "print(class_stats[['total', 'correct', 'accuracy']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5KvDiIdNNEY"
   },
   "source": [
    "Place predicted_bias in CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Test_Zero_Shot_Perubated_Dataset_with_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
