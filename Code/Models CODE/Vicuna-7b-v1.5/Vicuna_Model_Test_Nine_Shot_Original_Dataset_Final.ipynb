{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2h2_fO4g6kq1"
   },
   "source": [
    "1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1HroTx06tfy"
   },
   "source": [
    "2. Ensure Examples are correctly processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Policy: The job application process mandates that candidates disclose their age and retirement plans, with younger applicants prioritized for advancement.\n",
      "Classification: group_2\n",
      "\n",
      "Example 2\n",
      "Policy: Tax exemptions will be granted to corporations investing over $5 million in designated opportunity zones.\n",
      "Classification: group_2\n",
      "\n",
      "Example 3\n",
      "Policy: The bill excludes same‑sex partners from family leave benefits.\n",
      "Classification: group_2\n",
      "\n",
      "Example 4\n",
      "Policy: Immigrants must demonstrate fluency in English and pass a civic exam to be eligible for naturalization.\n",
      "Classification: group_1\n",
      "\n",
      "Example 5\n",
      "Policy: The policy mandates separate healthcare facilities for individuals with disabilities, citing operational constraints.\n",
      "Classification: group_1\n",
      "\n",
      "Example 6\n",
      "Policy: A bill mandates non‑citizens charged with a Class 4 felony shall be designated deportable immediately upon indictment.\n",
      "Classification: group_1\n",
      "\n",
      "Example 7\n",
      "Policy: The committee will meet quarterly to review environmental compliance reports.\n",
      "Classification: no_bias\n",
      "\n",
      "Example 8\n",
      "Policy: Applicants must submit identity verification and a signed consent form for record access.\n",
      "Classification: no_bias\n",
      "\n",
      "Example 9\n",
      "Policy: The funding request was approved following standard procurement guidelines and budgetary review.\n",
      "Classification: no_bias\n"
     ]
    }
   ],
   "source": [
    "def load_prompt(path: str) -> str:\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "prompt_text = load_prompt('/content/Examples_for_9_shot.txt')\n",
    "\n",
    "\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbYkwzR1fond"
   },
   "source": [
    "3. Load Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02dd72baf5242af9280601673391938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e8118332ef44e69cd9a9f13e1cbc6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5586e7671d1942a8a351c699d19e1481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9132f25af7994474b845d21d073ab356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8d9dc1eb57449d92ef6f06d16c615c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeef8cc7b163461e9c9c90f7d7393a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a14032a8c9942eab1808cce26f55a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c275506dba44c799237feb5e2fc1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ba53cd6cbe4df586c643c8c49d828c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a1ae4358954b4cb89b581e2991fae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9199d0824824186826cd71e74348f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/162 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = \"lmsys/vicuna-7b-v1.5\"\n",
    "device     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"left\"             # avoid right‑pad warning\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if device==\"cuda\" else -1,\n",
    "    return_full_text=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66GMmMMA69b2"
   },
   "source": [
    "4. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/content/FINAL_DATASET.csv\")\n",
    "bias_classes = [\"no_bias\",\"group_1\",\"group_2\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iP2yKcHX7DFx"
   },
   "source": [
    "5. Run Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ff63e578db4f6ea2d2ef52adb5336c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 49.05%\n"
     ]
    }
   ],
   "source": [
    "def build_prompt(description: str) -> str:\n",
    "    system = (\n",
    "        \"You are a specialized policy-analysis assistant.\\n\\n\"\n",
    "        \"You will give accurate and helpful answer to user input\\n\"\n",
    "        \"Definitions:\\n\"\n",
    "        \"- A bias policy targets, benefits, burdens, limits, or distinguishes a population based on identity, legal status, or economic position.\\n\"\n",
    "        \"- A non-bias policy is entirely procedural, factual, or operational, and does not involve any group-based treatment\\n\\n\"\n",
    "        \"- group_2 → Policies related to identity or socioeconomic status. Includes:\\n\"\n",
    "        \"- economic: mentions of income, poverty, unemployment assistance, financial assistance, or benefits based on economic standing. This includes housing policies (affordability, assistance programs, rent control), and employment group policies (such as policies affecting nurses or doctors).\\n\"\n",
    "        \"- racial/cultural: references to race, ethnicity, cultural communities, or related terminology. This includes any policy referencing ethnic groups, cultural practices, racial identities, or addressing racial disparities.\\n\"\n",
    "        \"- age: Any policy that refers to elderly individuals, youth, children, age-based programs or limitations. This includes incentive or help for seniors, youth programs, child welfare policies, or any regulation that treats age groups differently.\\n\"\n",
    "        \"- religion: any religious group, practice, or faith-based accommodation or restriction. This includes religious exemptions, exemption for place worship, or regulations affecting religious institutions.\\n\"\n",
    "        \"- gender: references to women, men, gender identity, sexual orientation, sexual discrimination, or gender-based rights. This includes sexual harassment policies, gender equity measures, reproductive rights, protections for gender expression, or LGBTQ+ considerations.\\n\\n\"\n",
    "        \"group_1 → Policies related to civic, institutional, or legal systems. Includes:\\n\"\n",
    "        \"- political: voting rights, voting procedures or regulations, elections, campaigns, or representation. This includes electoral processes, political participation, districting, governmental structures, or international relations.\\n\"\n",
    "        \"- criminal justice: law enforcement, courts, sentencing, prison, or rehabilitation. This includes policing policies, judicial proceedings, corrections, parole systems, or any aspect of the legal enforcement system.\\n\"\n",
    "        \"- citizenship: immigration status, visas, deportation, or naturalization. This includes border policies, citizenship requirements, asylum procedures, or voting rights related to citizenship status.\\n\"\n",
    "        \"- disability: accommodations or treatment for physical, mental, or learning impairments. This includes physical health accommodations, mental health services, accessibility requirements, or any policy that addresses different ability statuses.\\n\"\n",
    "        \"- education: curriculum, school placement, school procedures,learning standards, language proficiency, or educational access. This includes academic policies, language requirements in schools, educational testing, teacher credentials, or school enrollment procedures.\\n\\n\"\n",
    "        \"no_bias → Procedural, administrative, or operational text. Includes:\\n\"\n",
    "        \"- Scheduling, meeting logistics, budgeting language, implementation details, or general operations.\\n\"\n",
    "        \"- Technical definitions, purely factual information, or administrative processes that apply too all group.\\n\\n\"\n",
    "       \"Classification Procedure:\\n\"\n",
    "        \"1. Check for bias policy criteria using the definitions:\\n\"\n",
    "        \"   - Does the text ‘target, benefit, burden, limit, or distinguish’ a group based on identity (group_2) or legal/economic status (group_1)?\\n\"\n",
    "        \"2. If it meets bias criteria, apply the full category definitions above in priority order:\\n\"\n",
    "        \"   a. Search for any group_2 definition matches (economic, racial/cultural, age, religion, gender). If any apply → label `group_2`.\\n\"\n",
    "        \"   b. Otherwise, search for any group_1 definition matches (political, criminal justice, citizenship, disability, education). If any apply → label `group_1`.\\n\"\n",
    "        \"3. Within the chosen group, select the most specific subcategory when multiple definitions fit.\\n\"\n",
    "        \"4. Only if no group_1 or group_2 definitions apply → label `no_bias`.\\n\"\n",
    "        \"5. Output exactly one label (`group_2`, `group_1`, or `no_bias`) (With no punctuation or variations)\"\n",
    "        \"Here are examples of correct classifications for your reference; the order is unimportant:\\n\"\n",
    "        f\"{prompt_text}\"\n",
    "    )\n",
    "    user = (\n",
    "        f\"Classify the following policy excerpt.\\n\"\n",
    "        f\"Respond with *only one* of the following labels:(`group_2`, `group_1`, or `no_bias`) . No extra explanation or words:\\n\"\n",
    "        f\"Excerpt:\\n\\\"\\\"\\\"\\n{description}\\n\\\"\\\"\\\"\"\n",
    "    )\n",
    "\n",
    "    return f\"{system}\\nUSER: {user}\\nASSISTANT:\"\n",
    "\n",
    "\n",
    "def classify_batch(excerpts: list[str], batch_size: int = 32) -> list[str]:\n",
    "    prompts = [build_prompt(e) for e in excerpts]\n",
    "    preds: list[str] = []\n",
    "\n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Batches\"):\n",
    "        chunk = prompts[i:i+batch_size]\n",
    "\n",
    "        outputs = pipe(\n",
    "            chunk,\n",
    "            max_new_tokens=10,\n",
    "            batch_size=len(chunk),\n",
    "        )\n",
    "        # Flatten\n",
    "        flat = []\n",
    "        for item in outputs:\n",
    "            if isinstance(item, list):\n",
    "                flat.extend(item)\n",
    "            else:\n",
    "                flat.append(item)\n",
    "\n",
    "        for gen in flat:\n",
    "            text = gen[\"generated_text\"]\n",
    "            tok  = text.strip().split()[0].lower().replace(\"\\\\\", \"\")\n",
    "            preds.append(tok)\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "df[\"predicted_bias\"] = classify_batch(df[\"policy\"].tolist(), batch_size=8)\n",
    "df[\"correct\"] = (\n",
    "    df[\"bias_type_merged\"].str.strip().str.lower()\n",
    "    == df[\"predicted_bias\"]\n",
    ")\n",
    "print(f\"\\nAccuracy: {df['correct'].mean() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEjk9WZgjzxf"
   },
   "source": [
    "6. Show Ouput Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 49.05%\n",
      "\n",
      "Assembled metrics per class:\n",
      "\n",
      "         precision    recall  f1_score  support\n",
      "group_1   0.452144  0.650770  0.533572     7339\n",
      "group_2   0.520058  0.412972  0.460370     6969\n",
      "no_bias   0.543417  0.391012  0.454786     6386\n",
      "\n",
      "Per‑class accuracy (%):\n",
      "\n",
      "                  total  correct   accuracy\n",
      "bias_type_merged                           \n",
      "group_1            7339     4776  65.076986\n",
      "group_2            6969     2878  41.297173\n",
      "no_bias            6386     2497  39.101159\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "print(f\"\\nAccuracy: {df['correct'].mean() * 100:.2f}%\")\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    df['bias_type_merged'],\n",
    "    df['predicted_bias'],\n",
    "    labels=sorted(df['bias_type_merged'].unique()),\n",
    ")\n",
    "metrics_df = pd.DataFrame({\n",
    "    'precision': precision,\n",
    "    'recall':    recall,\n",
    "    'f1_score':  f1,\n",
    "    'support':   support\n",
    "}, index=sorted(df['bias_type_merged'].unique()))\n",
    "print(\"\\nAssembled metrics per class:\\n\")\n",
    "print(metrics_df)\n",
    "\n",
    "# Per‑class accuracy:\n",
    "class_stats = (\n",
    "    df\n",
    "    .groupby('bias_type_merged')\n",
    "    .agg(\n",
    "        total   = ('correct', 'count'),\n",
    "        correct = ('correct', 'sum')\n",
    "    )\n",
    ")\n",
    "class_stats['accuracy'] = class_stats['correct'] / class_stats['total'] * 100\n",
    "print(\"\\nPer‑class accuracy (%):\\n\")\n",
    "print(class_stats[['total', 'correct', 'accuracy']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaKmBCENL96t"
   },
   "source": [
    "7. Place predicted_bias in CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Test_Nine_Shot_Original_Dataset_with_predictions_.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
