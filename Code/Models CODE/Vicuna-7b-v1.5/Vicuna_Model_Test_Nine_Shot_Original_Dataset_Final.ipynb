{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2h2_fO4g6kq1"
   },
   "source": [
    "1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1HroTx06tfy"
   },
   "source": [
    "2. Ensure Examples are correctly processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt(path: str) -> str:\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "prompt_text = load_prompt('/content/Examples_for_9_shot.txt')\n",
    "\n",
    "\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbYkwzR1fond"
   },
   "source": [
    "3. Load Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"lmsys/vicuna-7b-v1.5\"\n",
    "device     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"left\"             # avoid right‑pad warning\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if device==\"cuda\" else -1,\n",
    "    return_full_text=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66GMmMMA69b2"
   },
   "source": [
    "4. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/content/FINAL_DATASET.csv\")\n",
    "bias_classes = [\"no_bias\",\"group_1\",\"group_2\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iP2yKcHX7DFx"
   },
   "source": [
    "5. Run Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(description: str) -> str:\n",
    "    system = (\n",
    "        \"You are a specialized policy-analysis assistant.\\n\\n\"\n",
    "        \"You will give accurate and helpful answer to user input\\n\"\n",
    "        \"Definitions:\\n\"\n",
    "        \"- A bias policy targets, benefits, burdens, limits, or distinguishes a population based on identity, legal status, or economic position.\\n\"\n",
    "        \"- A non-bias policy is entirely procedural, factual, or operational, and does not involve any group-based treatment\\n\\n\"\n",
    "        \"- group_2 → Policies related to identity or socioeconomic status. Includes:\\n\"\n",
    "        \"- economic: mentions of income, poverty, unemployment assistance, financial assistance, or benefits based on economic standing. This includes housing policies (affordability, assistance programs, rent control), and employment group policies (such as policies affecting nurses or doctors).\\n\"\n",
    "        \"- racial/cultural: references to race, ethnicity, cultural communities, or related terminology. This includes any policy referencing ethnic groups, cultural practices, racial identities, or addressing racial disparities.\\n\"\n",
    "        \"- age: Any policy that refers to elderly individuals, youth, children, age-based programs or limitations. This includes incentive or help for seniors, youth programs, child welfare policies, or any regulation that treats age groups differently.\\n\"\n",
    "        \"- religion: any religious group, practice, or faith-based accommodation or restriction. This includes religious exemptions, exemption for place worship, or regulations affecting religious institutions.\\n\"\n",
    "        \"- gender: references to women, men, gender identity, sexual orientation, sexual discrimination, or gender-based rights. This includes sexual harassment policies, gender equity measures, reproductive rights, protections for gender expression, or LGBTQ+ considerations.\\n\\n\"\n",
    "        \"group_1 → Policies related to civic, institutional, or legal systems. Includes:\\n\"\n",
    "        \"- political: voting rights, voting procedures or regulations, elections, campaigns, or representation. This includes electoral processes, political participation, districting, governmental structures, or international relations.\\n\"\n",
    "        \"- criminal justice: law enforcement, courts, sentencing, prison, or rehabilitation. This includes policing policies, judicial proceedings, corrections, parole systems, or any aspect of the legal enforcement system.\\n\"\n",
    "        \"- citizenship: immigration status, visas, deportation, or naturalization. This includes border policies, citizenship requirements, asylum procedures, or voting rights related to citizenship status.\\n\"\n",
    "        \"- disability: accommodations or treatment for physical, mental, or learning impairments. This includes physical health accommodations, mental health services, accessibility requirements, or any policy that addresses different ability statuses.\\n\"\n",
    "        \"- education: curriculum, school placement, school procedures,learning standards, language proficiency, or educational access. This includes academic policies, language requirements in schools, educational testing, teacher credentials, or school enrollment procedures.\\n\\n\"\n",
    "        \"no_bias → Procedural, administrative, or operational text. Includes:\\n\"\n",
    "        \"- Scheduling, meeting logistics, budgeting language, implementation details, or general operations.\\n\"\n",
    "        \"- Technical definitions, purely factual information, or administrative processes that apply too all group.\\n\\n\"\n",
    "       \"Classification Procedure:\\n\"\n",
    "        \"1. Check for bias policy criteria using the definitions:\\n\"\n",
    "        \"   - Does the text ‘target, benefit, burden, limit, or distinguish’ a group based on identity (group_2) or legal/economic status (group_1)?\\n\"\n",
    "        \"2. If it meets bias criteria, apply the full category definitions above in priority order:\\n\"\n",
    "        \"   a. Search for any group_2 definition matches (economic, racial/cultural, age, religion, gender). If any apply → label `group_2`.\\n\"\n",
    "        \"   b. Otherwise, search for any group_1 definition matches (political, criminal justice, citizenship, disability, education). If any apply → label `group_1`.\\n\"\n",
    "        \"3. Within the chosen group, select the most specific subcategory when multiple definitions fit.\\n\"\n",
    "        \"4. Only if no group_1 or group_2 definitions apply → label `no_bias`.\\n\"\n",
    "        \"5. Output exactly one label (`group_2`, `group_1`, or `no_bias`) (With no punctuation or variations)\"\n",
    "        \"Here are examples of correct classifications for your reference; the order is unimportant:\\n\"\n",
    "        f\"{prompt_text}\"\n",
    "    )\n",
    "    user = (\n",
    "        f\"Classify the following policy excerpt.\\n\"\n",
    "        f\"Respond with *only one* of the following labels:(`group_2`, `group_1`, or `no_bias`) . No extra explanation or words:\\n\"\n",
    "        f\"Excerpt:\\n\\\"\\\"\\\"\\n{description}\\n\\\"\\\"\\\"\"\n",
    "    )\n",
    "\n",
    "    return f\"{system}\\nUSER: {user}\\nASSISTANT:\"\n",
    "\n",
    "\n",
    "def classify_batch(excerpts: list[str], batch_size: int = 32) -> list[str]:\n",
    "    prompts = [build_prompt(e) for e in excerpts]\n",
    "    preds: list[str] = []\n",
    "\n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Batches\"):\n",
    "        chunk = prompts[i:i+batch_size]\n",
    "\n",
    "        outputs = pipe(\n",
    "            chunk,\n",
    "            max_new_tokens=10,\n",
    "            batch_size=len(chunk),\n",
    "        )\n",
    "        # Flatten\n",
    "        flat = []\n",
    "        for item in outputs:\n",
    "            if isinstance(item, list):\n",
    "                flat.extend(item)\n",
    "            else:\n",
    "                flat.append(item)\n",
    "\n",
    "        for gen in flat:\n",
    "            text = gen[\"generated_text\"]\n",
    "            tok  = text.strip().split()[0].lower().replace(\"\\\\\", \"\")\n",
    "            preds.append(tok)\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "df[\"predicted_bias\"] = classify_batch(df[\"policy\"].tolist(), batch_size=8)\n",
    "df[\"correct\"] = (\n",
    "    df[\"bias_type_merged\"].str.strip().str.lower()\n",
    "    == df[\"predicted_bias\"]\n",
    ")\n",
    "print(f\"\\nAccuracy: {df['correct'].mean() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEjk9WZgjzxf"
   },
   "source": [
    "6. Show Ouput Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "print(f\"\\nAccuracy: {df['correct'].mean() * 100:.2f}%\")\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    df['bias_type_merged'],\n",
    "    df['predicted_bias'],\n",
    "    labels=sorted(df['bias_type_merged'].unique()),\n",
    ")\n",
    "metrics_df = pd.DataFrame({\n",
    "    'precision': precision,\n",
    "    'recall':    recall,\n",
    "    'f1_score':  f1,\n",
    "    'support':   support\n",
    "}, index=sorted(df['bias_type_merged'].unique()))\n",
    "print(\"\\nAssembled metrics per class:\\n\")\n",
    "print(metrics_df)\n",
    "\n",
    "# Per‑class accuracy:\n",
    "class_stats = (\n",
    "    df\n",
    "    .groupby('bias_type_merged')\n",
    "    .agg(\n",
    "        total   = ('correct', 'count'),\n",
    "        correct = ('correct', 'sum')\n",
    "    )\n",
    ")\n",
    "class_stats['accuracy'] = class_stats['correct'] / class_stats['total'] * 100\n",
    "print(\"\\nPer‑class accuracy (%):\\n\")\n",
    "print(class_stats[['total', 'correct', 'accuracy']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaKmBCENL96t"
   },
   "source": [
    "7. Place predicted_bias in CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Test_Nine_Shot_Original_Dataset_with_predictions_.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
