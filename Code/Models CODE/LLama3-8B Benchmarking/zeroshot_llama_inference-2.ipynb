{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate pandas tqdm huggingface_hub\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=\"\") # insert own token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuO6vrRqvlf7"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "\n",
    "bias_classes = [\"group_2\", \"group_1\", \"no_bias\"]\n",
    "\n",
    "# Shared system prompt\n",
    "system_prompt = (\n",
    "    \"You are a policy-analysis assistant.\\n\\n\"\n",
    "    \"Your task is to classify each policy excerpt into **exactly one** of the following three categories, based on its main subject and social framing.\\n\\n\"\n",
    "\n",
    "    \"Understanding Bias:\\n\"\n",
    "    \"- A policy has a bias if it distinguishes, benefits, restricts, or treats people differently based on identity, legal status, or economic position.\\n\"\n",
    "    \"- This includes both: when a policy provides support, protections, or special resources for a group and when a policy excludes, limits, penalizes, or places extra burdens on a group.\\n\"\n",
    "    \"- Even if the wording is neutral, if any group is mentioned, implied, or treated differently, the policy has a bias.\\n\\n\"\n",
    "\n",
    "    \"Understanding No Bias:\\n\"\n",
    "    \"- A policy has no bias when it is entirely procedural, factual, or operational, and does not involve any group-based treatment.\\n\"\n",
    "    \"- No identity groups, legal status, or economic classifications should be mentioned or implied.\\n\"\n",
    "    \"- Examples include timelines, implementation steps, staff protocols, or technical logistics.\\n\\n\"\n",
    "\n",
    "    \"Classification Categories:\\n\"\n",
    "    \"group_2 → Policies involving personal identity or socioeconomic status:\\n\"\n",
    "    \"   - economic: poverty, income level, wealth, rent support, financial hardship, housing aid.\\n\"\n",
    "    \"   - racial/cultural: race, ethnicity, cultural identity, minority status, racial disparities.\\n\"\n",
    "    \"   - age: elderly, children, youth-targeted policies, age-based restrictions or services.\\n\"\n",
    "    \"   - religion: religious practices, faith-based exceptions, worship protections.\\n\"\n",
    "    \"   - gender: women's rights, gender identity, gender discrimination, reproductive policy.\\n\\n\"\n",
    "    \"group_1 → Policies focused on legal, civic, or institutional treatment:\\n\"\n",
    "    \"   - political: elections, campaigning, districting, war related, international realtions, veteran related.\\n\"\n",
    "    \"   - criminal justice: policing, sentencing, incarceration, parole, law enforcement.\\n\"\n",
    "    \"   - citizenship: immigration status, voting rights, naturalization, deportation, visa policy.\\n\"\n",
    "    \"   - disability: medical, accommodations, physical/mental impairments, special education services.\\n\"\n",
    "    \"   - education: student placement, curriculum, testing access, teaching, education access, language proficiency, school enrollment.\\n\\n\"\n",
    "    \"no_bias → Neutral, operational, or internal content with no group implications:\\n\"\n",
    "    \"   - Meeting schedules, titles, definitions, procedural instructions, timelines, internal processes, technical steps.\\n\\n\"\n",
    "\n",
    "    \"Classification Rules:\\n\"\n",
    "    \"- If a group is named, implied, or treated differently, assign a bias label (group_1 or group_2) based on the domain.\\n\"\n",
    "     \"- A policy has a bias if it distinguishes, benefits, restricts, or treats people differently based on identity, legal status, or economic position.\\n\"\n",
    "    \"- A policy has no bias when it is entirely procedural, factual, or operational, and does not involve any group-based treatment.\\n\"\n",
    "    \"- Do not default to any label. Treat all three categories as equally likely.\\n\"\n",
    "    \"- Pick the most specific match based on content.\\n\"\n",
    "    \"- Return only one label: `no_bias`, `group_1`, or `group_2` — all lowercase, no punctuation, no explanation.\"\n",
    ")\n",
    "\n",
    "\n",
    "def classify_batch(df, batch_size=32, column=\"policy\"):\n",
    "    all_preds = []\n",
    "    num_batches = ceil(len(df) / batch_size)\n",
    "\n",
    "    for i in tqdm(range(num_batches), desc=\"Classifying in batches\"):\n",
    "        batch_df = df.iloc[i * batch_size : (i + 1) * batch_size]\n",
    "        prompts = []\n",
    "\n",
    "        for description in batch_df[column]:\n",
    "            user_prompt = (\n",
    "                f\"Classify the following policy excerpt.\\n\"\n",
    "                f\"Return only one of: no_bias, group_1, or group_2\\n\\n\"\n",
    "                f\"Excerpt:\\n\\\"\\\"\\\"\\n{description}\\n\\\"\\\"\\\"\\n\\n\"\n",
    "                f\"Your answer:\"\n",
    "            )\n",
    "\n",
    "            full_prompt = tokenizer.apply_chat_template([\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "            prompts.append(full_prompt)\n",
    "\n",
    "        # Run batched generation\n",
    "        outputs = pipe(prompts, max_new_tokens=20, do_sample=False)\n",
    "\n",
    "        for output, prompt in zip(outputs, prompts):\n",
    "            generated = output[0]['generated_text']\n",
    "            prediction = generated[len(prompt):].strip().split(\"\\n\")[0].lower()\n",
    "\n",
    "            normalized_pred = (\n",
    "                prediction.replace(\"_\", \"\")\n",
    "                          .replace(\"-\", \"\")\n",
    "                          .replace(\".\", \"\")\n",
    "                          .replace(\"(\", \"\")\n",
    "                          .replace(\")\", \"\")\n",
    "                          .strip()\n",
    "            )\n",
    "\n",
    "            label = \"unknown\"\n",
    "            for cls in bias_classes:\n",
    "                if normalized_pred == cls.replace(\"_\", \"\"):\n",
    "                    label = cls\n",
    "                    break\n",
    "\n",
    "            all_preds.append(label)\n",
    "\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define output directory (overwrite okay)\n",
    "output_dir = \"/content/drive/MyDrive/policy_bias_checkpoints\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load full dataset\n",
    "df = pd.read_csv(\"FINAL_DATASET.csv\")     \n",
    "\n",
    "# Create an empty column to hold your running predictions\n",
    "df['predicted_bias'] = None\n",
    "\n",
    "batch_size  = 32                         \n",
    "num_batches = ceil(len(df) / batch_size)\n",
    "\n",
    "for batch_i in tqdm(range(num_batches), desc=\"Classifying and checkpointing\"):\n",
    "    start = batch_i * batch_size\n",
    "    end   = min(start + batch_size, len(df))\n",
    "\n",
    "    # slice out just this batch\n",
    "    batch_idx = df.index[start:end]\n",
    "    batch_df  = df.loc[batch_idx]\n",
    "\n",
    "    # run your classifier on just this slice\n",
    "    preds = classify_batch(batch_df, batch_size=batch_size)\n",
    "\n",
    "    # write those back into your master DataFrame\n",
    "    df.loc[batch_idx, 'predicted_bias'] = preds\n",
    "\n",
    "    # now save the entire “so far” (or the full df) to a checkpoint\n",
    "    ckpt_name = f\"checkpoint_up_to_{end}.csv\"\n",
    "    df.to_csv(os.path.join(output_dir, ckpt_name), index=False)\n",
    "    print(f\"  → saved full table with predictions through row {end} to {ckpt_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Define valid classes\n",
    "valid_labels = [\"group_2\", \"group_1\", \"no_bias\"]\n",
    "\n",
    "# Clean up casing and spacing\n",
    "df['bias_type_merged'] = df['bias_type_merged'].astype(str).str.strip().str.lower()\n",
    "df['predicted_bias'] = df['predicted_bias'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Filter to rows with valid labels only\n",
    "df_clean = df[\n",
    "    df['predicted_bias'].isin(valid_labels) &\n",
    "    df['bias_type_merged'].isin(valid_labels)\n",
    "].copy()\n",
    "\n",
    "# Accuracy\n",
    "df_clean['correct'] = df_clean['bias_type_merged'] == df_clean['predicted_bias']\n",
    "accuracy = df_clean['correct'].mean()\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Precision, Recall, F1\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    df_clean['bias_type_merged'],\n",
    "    df_clean['predicted_bias'],\n",
    "    labels=valid_labels,\n",
    "    digits=3\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"predicted_bias\"].unique())\n",
    "print(df[\"predicted_bias\"].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
