# -*- coding: utf-8 -*-
"""qwen_inference_zeroshot_normal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q1-sBuOYWDm0qa49h_7WkGHwxJQmdlme
"""

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from tqdm import tqdm
import os
import time
from tqdm.notebook import tqdm
from datetime import timedelta
from google.colab import drive

# Set up progress bar for pandas
tqdm.pandas()

# Mount Google Drive first (only needs to be run once per session)
drive.mount('/content/drive')

# Define a path on your Google Drive to save results
DRIVE_PATH = "/content/drive/MyDrive/bias_classification_results"  # Change this to your desired folder
os.makedirs(DRIVE_PATH, exist_ok=True)  # Create the directory if it doesn't exist
output_path = f"{DRIVE_PATH}/bias_classification_results.csv"

# Install required packages
!pip install -q transformers accelerate pandas tqdm huggingface_hub

# Login to Hugging Face
from huggingface_hub import login
login(token="")

# Define model and device
model_name = "Qwen/Qwen2.5-7B-Instruct"
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
    device_map="auto"
)

# Create pipeline
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Load dataset
df = pd.read_csv("FINAL_DATASET.csv")
bias_classes = ["no_bias", "group_1", "group_2"]

def classify_bias(description):
    system_prompt = (
        "You are a specialized policy-analysis assistant.\n\n"
        "Definitions:\n"
        "- A bias policy targets, benefits, burdens, limits, or distinguishes a population based on identity, legal status, or economic position.\n"
        "- A non-bias policy is entirely procedural, factual, or operational, and does not involve any group-based treatment\n\n"
        "Categories:\n"
        "group_2 → Policies related to identity or socioeconomic status. Includes:\n"
        "- economic: mentions of income, poverty, wealth, financial assistance, housing, or benefits based on economic standing. This includes housing policies (affordability, assistance programs, rent control), and employment policies (wage regulations, job protections based on economic factors).\n"
        "- racial/cultural: references to race, ethnicity, cultural communities, or related terminology. This includes any policy referencing ethnic groups, cultural practices, racial identities, or measures addressing racial disparities.\n"
        "- age: refers to elderly individuals, youth, children, age-based programs or limitations. This includes specific protections for seniors, youth programs, child welfare policies, or any regulation that treats age groups differently.\n"
        "- religion: any religious group, practice, or faith-based accommodation or restriction. This includes religious exemptions, protections for worship, faith-based considerations in public policy, or regulations affecting religious institutions.\n"
        "- gender: references to women, men, gender identity, sexual orientation, sexual discrimination, or gender-based rights. This includes sexual harassment policies, gender equity measures, reproductive rights, protections for gender expression, or LGBTQ+ considerations.\n\n"
        "group_1 → Policies related to civic, institutional, or legal systems. Includes:\n"
        "- political: voting rights, elections, campaigns, or representation. This includes electoral processes, political participation, districting, governmental structures, or international relations.\n"
        "- criminal justice: law enforcement, courts, sentencing, prison, or rehabilitation. This includes policing policies, judicial proceedings, corrections, parole systems, or any aspect of the legal enforcement system.\n"
        "- citizenship: immigration status, visas, deportation, or naturalization. This includes border policies, citizenship requirements, asylum procedures, or voting rights related to citizenship status.\n"
        "- disability: accommodations or treatment for physical, mental, or learning impairments. This includes physical health accommodations, mental health services, accessibility requirements, or any policy that addresses different ability statuses.\n"
        "- education: curriculum, school placement, learning standards, language proficiency, or educational access. This includes academic policies, language requirements in schools, educational testing, teacher credentials, or school enrollment procedures.\n\n"
        "no_bias → Procedural, administrative, or operational text. Includes:\n"
        "- Scheduling, meeting logistics, budgeting language, implementation details, or general operations with no reference to any group.\n"
        "- Technical definitions, purely factual information, or administrative processes that apply universally.\n\n"
        "Guidelines:\n"
        "- If the policy treats or discusses a group based on identity or economic status → group_2.\n"
        "- If it focuses on legal rights, school policies, enforcement, or institutional rules → group_1.\n"
        "- If it is neutral, factual, with no identity or legal relevance → no_bias.\n"
        "- Do not default to any label. All three labels are equally likely.\n"
        "- Always choose the most specific match, even if identity is only implied.\n"
        "- Output only the label: no_bias, group_1, or group_2."
    )

    user_prompt = (
        f"Classify the following policy excerpt.\n\n"
        f"Excerpt:\n\"\"\"\n{description}\n\"\"\"\n\n"
        f"Follow these exact steps in order:\n"
        f"Step 1: Does it involve ANY of these themes:\n"
        f"- Economic status (income, poverty, wealth, financial aid, housing support)\n"
        f"- Racial or cultural identity (race, ethnicity, minority status)\n"
        f"- Age groups (elderly, children, youth, seniors)\n"
        f"- Religious practices or faith-based considerations\n"
        f"- Gender or gender identity (women, men, transgender, LGBTQ+)\n"
        f"If YES to ANY of these → group_2\n\n"
        f"Step 2: If not group_2, does it involve ANY of these themes:\n"
        f"- Education (schools, students, teachers, curriculum)\n"
        f"- Political matters (voting, elections, government functions)\n"
        f"- Criminal justice (law enforcement, courts, incarceration)\n"
        f"- Citizenship (immigration, visas, naturalization)\n"
        f"- Disability services (accommodations, accessibility)\n"
        f"If YES to ANY of these → group_1\n\n"
        f"Step 3: If neither group_2 nor group_1, then it must be purely procedural → no_bias\n\n"
        f"Classification (return ONLY 'no_bias', 'group_1', or 'group_2'):"
    )


    # Apply the Qwen2.5 chat template
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # Generate response
    outputs = pipe(prompt, max_new_tokens=10, do_sample=False)
    generated = outputs[0]['generated_text']
    raw_output = generated[len(prompt):].strip()
    prediction = raw_output.split("\n")[0].lower()

    # Extract the actual bias classification
    found_class = "unknown"
    for cls in bias_classes:
        if cls in prediction:
            found_class = cls
            break

    return found_class, raw_output

# Create a mapping from specific categories to their group labels
category_to_group = {
    # group_1 categories
    "political": "group_1",
    "criminal_justice": "group_1",
    "citizenship": "group_1",
    "disability": "group_1",
    "education": "group_1",

    # group_2 categories
    "economic": "group_2",
    "racial_cultural": "group_2",
    "age": "group_2",
    "religion": "group_2",
    "gender": "group_2",

    # no_bias remains the same
    "no_bias": "no_bias"
}

# Create a function to save results
def save_batch(batch_df, output_path=output_path):
    # Check if file exists to determine if we need headers
    header = not os.path.exists(output_path)
    batch_df.to_csv(output_path, mode='a', header=header, index=False)
    print(f"✓ Saved {len(batch_df)} rows to Drive")

# Define batch size and calculate batches
batch_size = 32
total_rows = len(df)
total_batches = (total_rows + batch_size - 1) // batch_size
print(f"Processing {total_rows} rows in {total_batches} batches")

# Check if we have previous results to resume from
if os.path.exists(output_path):
    existing_results = pd.read_csv(output_path)

    # Check if the CSV has the expected columns
    expected_columns = ['policy', 'bias_type', 'predicted_bias', 'raw_output', 'bias_type_group', 'correct']
    if all(col in existing_results.columns for col in expected_columns):
        rows_already_processed = len(existing_results)
        start_batch = rows_already_processed // batch_size

        if start_batch > 0:
            print(f"Found existing results with {rows_already_processed} rows.")
            print(f"Last processed batch was {start_batch}")
            print(f"Resuming from batch {start_batch+1}/{total_batches}")
    else:
        print("Warning: Existing results file has unexpected format. Starting from beginning.")
        start_batch = 0
else:
    start_batch = 0
    print("No existing results found. Starting from the beginning.")

# Initialize timing
start_time = time.time()

# Initialize overall accuracy tracking
total_correct = 0
total_processed = 0

# If resuming, get the current overall statistics
if start_batch > 0 and os.path.exists(output_path):
    existing_results = pd.read_csv(output_path)
    if 'correct' in existing_results.columns:
        total_correct = existing_results['correct'].sum()
        total_processed = len(existing_results)
        initial_accuracy = (total_correct / total_processed) * 100
        print(f"Initial overall accuracy: {initial_accuracy:.2f}% ({total_processed} rows)")

# Create a tqdm progress bar for tracking batches
# Adjust total to only count remaining batches
remaining_batches = total_batches - start_batch
batch_progress = tqdm(total=remaining_batches, desc="Overall Progress")

# Process all batches, starting from where we left off
for batch_idx in range(start_batch, total_batches):
    batch_start = time.time()

    # Get current batch
    start_idx = batch_idx * batch_size
    end_idx = min(start_idx + batch_size, total_rows)
    batch_df = df.iloc[start_idx:end_idx].copy()

    # Show batch info
    print(f"\nBatch {batch_idx+1}/{total_batches} (rows {start_idx+1}-{end_idx})")

    # Show time estimates if not the first batch
    if batch_idx > start_batch:
        elapsed = time.time() - start_time
        avg_time_per_batch = elapsed / (batch_idx - start_batch)
        remaining = avg_time_per_batch * (total_batches - batch_idx)

        # Display time info
        print(f"Elapsed: {str(timedelta(seconds=int(elapsed)))}")
        print(f"Remaining: {str(timedelta(seconds=int(remaining)))}")
        print(f"Est. completion: {time.strftime('%H:%M:%S', time.localtime(start_time + elapsed + remaining))}")

    # Process batch
    results = []
    for idx, row in enumerate(batch_df.itertuples()):
        result = classify_bias(row.policy)
        results.append(result)

    batch_df['predicted_bias'] = [r[0] for r in results]
    batch_df['raw_output'] = [r[1] for r in results]

    # Map categories and calculate accuracy
    batch_df['bias_type_group'] = batch_df['bias_type'].str.strip().str.lower().map(category_to_group)
    batch_df['correct'] = batch_df['bias_type_group'] == batch_df['predicted_bias']
    batch_accuracy = batch_df['correct'].mean() * 100

    # Update overall statistics
    batch_correct = batch_df['correct'].sum()
    batch_size_actual = len(batch_df)
    total_correct += batch_correct
    total_processed += batch_size_actual
    overall_accuracy = (total_correct / total_processed) * 100

    # Save to Drive
    save_batch(batch_df)

    # Print accuracy information
    print(f"Batch accuracy: {batch_accuracy:.2f}% | Overall accuracy: {overall_accuracy:.2f}% ({total_processed} rows)")

    # Update batch progress bar
    batch_progress.update(1)
    batch_progress.set_postfix({
        "Batch Acc": f"{batch_accuracy:.1f}%",
        "Overall Acc": f"{overall_accuracy:.1f}%",
        "Rows": f"{end_idx}/{total_rows}"
    })

import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix

# Load the results dataset
df = pd.read_csv("bias_classification_results.csv")

# Assuming the columns are named 'bias_type_group' for true labels and 'predicted_bias' for predictions
# If they have different names, adjust these variables
true_column = 'bias_type_group'  # Column containing true labels
pred_column = 'predicted_bias'   # Column containing model predictions

# Generate and print classification report
print("Classification Report:")
report = classification_report(df[true_column], df[pred_column])
print(report)

# Generate confusion matrix
conf_matrix = confusion_matrix(df[true_column], df[pred_column])
classes = sorted(df[true_column].unique())
conf_df = pd.DataFrame(conf_matrix, index=classes, columns=classes)
print("\nConfusion Matrix:")
print(conf_df)

# Calculate overall accuracy
accuracy = (df[true_column] == df[pred_column]).mean()
print(f"\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

# Distribution of true labels and predictions
print("\nDistribution of True Labels:")
print(df[true_column].value_counts())

print("\nDistribution of Predicted Labels:")
print(df[pred_column].value_counts())