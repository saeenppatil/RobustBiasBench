{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/kevinxu/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from huggingface_hub import login\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Log in to huggingface to use Mistral model\n",
    "login(\"\")\n",
    "\n",
    "# Bias classes of dataset\n",
    "bias_classes = [\n",
    "    \"no_bias\", \"group_1\", \"group_2\"\n",
    "]\n",
    "\n",
    "# Load dataset with perturbed policy texts\n",
    "df = pd.read_csv(\"FINAL_PERTURBED_DATASET.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Mistral model and tokenizer\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Mistral to predict the bias of a policy\n",
    "def classify_bias(description):\n",
    "    # Prompt for zero-shot prediction with Mistral\n",
    "    prompt = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": (f\"You are a policy-analysis assistant.\\n\"\n",
    "                     f\"Given a policy excerpt, output *exactly one* of these three labels—nothing else, all lower-case:\\n\"\n",
    "                     f\"- group_2:\\n\"\n",
    "                     f\"- group_1:\\n\"\n",
    "                     f\"- no_bias\\n\\n\"\n",
    "                     f\"Definitions:\\n\"\n",
    "                     f\"- group_2: policies related to identity, social, and economic status, including:\\n\"\n",
    "                     f\"• economic: references income, poverty, homelessness, wealth, financial assistance, costs, payments, fees, funding, housing, area living in, etc.\\n\"\n",
    "                     f\"• racial_cultural: references race, ethnicity, culture, personal beliefs, etc.\\n\"\n",
    "                     f\"• age: references children, youth, child welfare policies, adults, elderly, seniors, age-related policies, etc.\\n\"\n",
    "                     f\"• religion: references religious beliefs, regligious groups, faith-based accommodations, etc.\\n\"\n",
    "                     f\"• gender: references women, men sex, gender identity, use of only one pronoun for a role, sexual harassment, reproductive rights, pregnancy, etc.\\n\"\n",
    "                     f\"- group_1: policies related to legal, institutional, or civic systems, including:\\n\"\n",
    "                     f\"• political: references voting rights, politics, elections, campaigns, government, war, international relations, etc.\\n\"\n",
    "                     f\"• criminal_justice: refernces crime, criminals, court, law enforcement, policing, prison, etc.\\n\"\n",
    "                     f\"• citizenship: references immigration, immigration status, deportation, visas, border control, etc.\\n\"\n",
    "                     f\"• disability: references physical or mental impairments, accommodations for impairments, accessibility, illness, etc.\\n\"\n",
    "                     f\"• education: curriculum, degrees, teaching credentials, language proficiency, language required to learn, standardized testing, school admission, etc.\\n\"\n",
    "                     f\"- no_bias: procedural, definitional, administrative, factual, or operational text, like implementation details or definitions.\\n\\n\"\n",
    "                     f\"Instructions:\\n\"\n",
    "                     f\"Follow the following steps exactly, when picking a label:\\n\"\n",
    "                     f\"Step 1: Does the excerpt fit under economic, racial_cultural, age, religion, or gender? If yes, pick group_2.\\n\"\n",
    "                     f\"Step 2: If the answer to Step 1 was \\\"no\\\", does the excerpt fit under political, criminal_justice, citizenship, disability, or education? If yes, pick group_1.\\n\"\n",
    "                     f\"Step 3: If the excerpt does not fit under group_1 or group_2, it is purely a procedure, a definition, or administrative, so pick no_bias.\\n\"\n",
    "                     f\"- Do not default to any label. All labels are equally possible for an excerpt.\\n\"\n",
    "                     f\"- Return just the label; no explanations.\"\n",
    "                     )},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": (f\"Classify the following policy excerpt.\\n\"\n",
    "                     f\"Return *only* one of: no_bias, group_1, group_2\\n\\n\"\n",
    "                     f\"Excerpt:\\n\"\n",
    "                     f\"\\\"\\\"\\\"\\n\"\n",
    "                     f\"{description}\\n\"\n",
    "                     f\"\\\"\\\"\\\"\"\n",
    "                     )}\n",
    "    ]\n",
    "    \n",
    "    # Get predicted bias classification from model\n",
    "    result = pipe(prompt, max_new_tokens=10, do_sample=False)\n",
    "    generated_text = result[0]['generated_text']\n",
    "    predicted = generated_text[-1][\"content\"].strip().split(\"\\n\")[0]\n",
    "\n",
    "    for cls in bias_classes:\n",
    "        if cls.lower() in predicted.lower():\n",
    "            return cls\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Mistral to predict bias of every policy\n",
    "for i in tqdm(range(len(df))):\n",
    "    prediction = classify_bias(df.loc[i, \"policy_perturbed\"])\n",
    "    df.loc[i, \"predicted_bias_perturbed\"] = prediction\n",
    "    df.to_csv(\"mistral_zero_shot_perturbed.csv\", index = False)\n",
    "\n",
    "# Calculate the number of correct bias predictions\n",
    "df['correct_perturbed'] = df['bias_type_merged'].str.strip().str.lower() == df['predicted_bias_perturbed'].str.strip().str.lower()\n",
    "\n",
    "df.to_csv(\"mistral_zero_shot_perturbed.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 47.33%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     group_1       0.63      0.22      0.33      7339\n",
      "     group_2       0.44      0.69      0.54      6969\n",
      "     no_bias       0.47      0.53      0.50      6386\n",
      "\n",
      "    accuracy                           0.47     20694\n",
      "   macro avg       0.51      0.48      0.45     20694\n",
      "weighted avg       0.52      0.47      0.45     20694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation results for Mistral bias predictions\n",
    "df = pd.read_csv(\"mistral_zero_shot_perturbed.csv\")\n",
    "accuracy = df['correct_perturbed'].mean()\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "labels = df[\"bias_type_merged\"].tolist()\n",
    "predictions = df[\"predicted_bias_perturbed\"].tolist()\n",
    "\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
